{
  "hash": "a005546666c202e8b4bc573837d53de8",
  "result": {
    "markdown": "---\ntitle: \"Exercice 01\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-02-xx\"\ndate-modified: \"2025-02-27\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"categorie 1\", \"cotegorie 2\"]\nimage: \"\"\ndescription: \"Description\"\n---\n\n\n# Intervenant.e.s\n\n### Rédaction\n\n-   **Clément Poupelin**, [clementjc.poupelin\\@gmail.com](mailto:clementjc.poupelin@gmail.com){.email}\\\n\n### Relecture\n\n-   \n\n# Setup\n\n:::: panel-tabset\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Données\nlibrary(dplyr)        # manipulation des données\n\nlibrary(latex2exp)\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n:::\n\n\n## Fonctions\n\n::: panel-tabset\n### Fonction 1\n\n### Fonction 2\n:::\n\n## Seed\n::::\n\n# Données\n\n# Analyse\n\n::: callout-note\nMETTRE LES REMARQUES\n:::\n\n::: callout-warning\nMETTRE LES POINTS D'ATTENTION\n:::\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nMETTRE LES CONCLUSIONS\n:::\n\n# Conclusion\n\n# Session info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n latex2exp * 0.9.6   2022-11-28 [1] CRAN (R 4.2.3)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n\n\n\n\n\n# **EXERCICE 1 : **\n<br>\n\n\nNous étudions la différence entre une marche aléatoire et un signal linéaire bruité.\n<br>\n\n\n#### QUESTION 1 : Simuler dix marches aléatoires $(x_t)_t = \\delta + x_{t-1} + w_t$ avec dérive de longueur $n = 100$, de paramètre $\\delta = .01$ et de variance $\\sigma^2_W = 1$ pour le bruit.\n<br>\n\n\nSi on pose que $x_0 = w_0$, on peut écrire notre marche aléatoire comme $x_t = \\delta t + \\sum_{i=0}^{t}w_i$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# On pose nos paramètres\nn = 100         \ndelta = 0.01    \n\n# On définis notre fonction de marche aléatoire\nrandom_walk = function(n, delta) {\n  w = rnorm(n)  \n  drift = delta * seq(1, n)  \n  \n  x = drift + cumsum(w)\n  return(x)  \n}\n\n# Générer dix marches aléatoires\nnb = 10\nsim = matrix(0, ncol = n, nrow = nb)\nfor (i in 1:nb) {\n  sim[i, ] = random_walk(n, delta)\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n#### QUESTION 2 : Estimer le modèle de régression linéaire $x_t = \\beta_t + w_t$\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist=1:n\nl = c()\n\nfor (i in 1:nb){\n  mod = lm(sim[i,] ~ list + 0) \n  # +0 pour ne pas faire de modèle avec constante\n  l[i] = mod$coefficients\n}\n```\n:::\n\n\n\n\n<br>\n\n#### QUESTION 3 : Représenter sur un même graphique les dix droites estimées et la tendance moyenne théorique $\\delta_t = .01t$\n\n<br>\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 4 : Simuler dix séries $(x_t)_t$ de la forme $x_t = \\delta_t + w_t$ (tendance+bruit blanc) de longueur $n = 100$, de paramètre $\\delta = .01$ et de variance $\\sigma^2_W = 1$\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# On définis notre signal linéaire bruité \nnoisy_serie = function(n, delta) {\n  w = rnorm(n, sd = 1)  \n  drift = delta * seq(1, n) \n  \n  x = drift + w\n  return(x)  \n}\n\n# Générer dix séries tendance + bruit\nsim2 = matrix(0, ncol = n, nrow = nb)\n\nfor (i in 1:nb) {\n  sim2[i, ] = noisy_serie(n, delta)\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n<br>\n\n#### QUESTION 5 : Estimer le modèle de régression linéaire $x_t = \\beta_t + w_t$\n<br>\n\n::: {.cell}\n\n```{.r .cell-code}\nl2 = c()\nfor (i in 1:nb){\n  mod2 = lm(sim2[i,] ~ list + 0)\n  l2[i] = mod2$coefficients\n}\n```\n:::\n\n<br>\n\n#### QUESTION 6 : Représenter sur un même graphique les dix droites estimées et la tendance théorique $\\delta_t = .01t$\n<br>\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 7 : Commenter les résultats\n<br>\n\nLa tendance théorique (le drift) est mieux estimée par régression dans le cas d'un signal bruité que celui de la marche aléatoire. \n\nCela peut s'explique par le fait que, dans le cas de la marche aléatoire, la variance de $x_t$ croît linéairement avec le temps. En effet, \n$Var(x_t) = Var( \\delta t + \\sum_{i=0}^{t}w_i) = Var( \\sum_{i=0}^{t}w_i)=\\sum_{i=0}^{t}Var(w_i) = \\sum_{i=0}^{t}\\sigma^2_w=t\\sigma^2_w$\n\n\nCela fait donc défaut à l'hypothèse d'homoscédacité cruciale pour la régression linéaire \n\nPar contre, du côté du signal bruite on conserve l'homoscédacité avec le cas très idéal du bruit iid et gaussien.\n<br>\n\n# **EXERCICE 2 : **\n<br>\n\n#### QUESTION 1 : Écrire une fonction qui retourne une série simulée de la forme $X_j = a cos(ω_j) + bj + \\varepsilon_j$ où $(\\varepsilon_n)$ un bruit blanc gaussien centré et de variance 1.\n<br>\n\nLes paramètres d’entrée de la fonction sont $n$, $a$, $b$, $w$ et la sortie est une série temporelle. Pour cela, on utilise la fonction ts() qui transforme nos points générés en une série temporelle.\n\n::: {.cell}\n\n```{.r .cell-code}\nX_j = function(n, a, b, w) {\n  eps = rnorm(n)  \n  \n  x = a*cos(w*seq(1, n) ) + b* seq(1, n) +eps\n  return(ts(x))   \n}\n```\n:::\n\n\n\nMaintenant, on fixe $n = 100$ puis $n = 500$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = c(100, 500)\n```\n:::\n\n\nPuis on pose les paramètres qui nous serons utiles par la suite\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = c(0, 2)\nb = c(0.01, 0)\nw = c(2*pi, pi/6) \n#en 2*pi, w n'aura pas d'influence si on voulait enlever la condition a = 0\n```\n:::\n\n<br>\n\n#### QUESTION 2 : Pour $a = 0$ et $b = .01$, simuler une trajectoire, puis représenter\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pour n = 100\nsim1_X_j_100 = X_j(n[1], a[1], b[1], w[1])\n\n# Pour n = 500\nsim1_X_j_500 = X_j(n[2], a[1], b[1], w[1])\n```\n:::\n\n<br>\n\n#### 2-1 : la série et sa suite d’auto-corrélations empiriques\n<br>\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### 2-2 : la série $X_n − X_{n−1}$ et sa suite des auto-corrélations empiriques\n<br>\n\n::: {.cell}\n\n```{.r .cell-code}\nsim1_X_j_100_diff = diff(sim1_X_j_100, lag = 1)\n\nsim1_X_j_500_diff = diff(sim1_X_j_500, lag = 1)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 3 : Pour $b = 0$, $a = 2$ et $w = \\frac{\\pi}{6}$, simuler une trajectoire, puis représenter\n<br>\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pour n = 100\nsim2_X_j_100 = X_j(n[1], a[2], b[2], w[2])\n\n# Pour n = 500\nsim2_X_j_500 = X_j(n[2], a[2], b[2], w[2])\n```\n:::\n\n<br>\n\n#### 3-1 : la série et sa suite des auto-corrélations empiriques\n<br>\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n<br>\n\n#### 3-2 : la série $X_n − X_{n−12}$ et sa suite des auto-corrélations empiriques\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim2_X_j_100_diff = diff(sim2_X_j_100, lag = 12)\n\nsim2_X_j_500_diff = diff(sim2_X_j_500, lag = 12)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\nPour conclure, on peut constater qu'en grandissant l'échantillon, la tendance et la saisonnalité ressortent d'avantage et influencent nos autocorrélation. On ne pourra donc pas considérer les séries comme stationnaires. \nMais, l'opération de différenciation peut permettre de résoudre ce problème d'autocorrélation quand celle ci est adaptée à la \"perturbation\"(tendance ou saison) de notre série. \n\nPour visualiser cela, on peut effecuer les calculs.\nDans le cas de la série $X_j = 0.01j +\\varepsilon_j$ Où il reste l'effet de tendance, \n\\begin{align*}\nX_j - X_{j-1} &= 0.01j +\\varepsilon_j - 0.01(j-1) -\\varepsilon_{j-1}\\\\\n &= 0.01j - 0.01j - 0.01  + \\varepsilon_j  - \\varepsilon_{j-1}\\\\\n &= -0.01  +\\varepsilon_j  - \\varepsilon_{j-1}\\\\\n\\end{align*}\nOn a bien une disparition de la tendance.\n\n\nDans le cas de la série $X_j = 2cos(\\frac{\\pi}{6}j) + \\varepsilon_j$ Où il reste l'effet de saison, \n\\begin{align*}\nX_j - X_{j-12} &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}(j-12)) -\\varepsilon_{j-12}\\\\\n &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}j-\\frac{\\pi}{6}12) -\\varepsilon_{j-12}\\\\\n &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}j-2\\pi) -\\varepsilon_{j-12}\\\\\n &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2(cos(\\frac{\\pi}{6}j)cos(2\\pi) - sin(\\frac{\\pi}{6}j)sin(2\\pi)) -\\varepsilon_{j-12}\\\\\n &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2(cos(\\frac{\\pi}{6}j) - 0) -\\varepsilon_{j-12}\\\\\n &= \\varepsilon_j -\\varepsilon_{j-12}\\\\\n\\end{align*}\nOn a bien une disparition de la saison.\n\n<br>\n\nOn a donc qu'une différentiation d'ordre 1 permettra d'enlever la tendance et une différentiation d'ordre $s$ permettra d'enlever une saison de période $s$.\n\n\n<br>\n\n\n# **EXERCICE 3 : **\n<br>\n\n#### QUESTION 1 : Ecrire une fonction pour simuler des trajectoires de processus défini par l’équation de récurrence $X_m + cX_{m−1} = \\varepsilon_m$ où $(\\varepsilon_m)$ est une suite de variables aléatoires centrées iid.\n<br>\n\n**Indication** : Pour obtenir une série de longueur $m$, simuler $m+ 100$ valeurs et supprimer les\n100 premières valeurs pour atténuer l’effet de l’initialisation. Vous pouvez utiliser la fonction filter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxt = function(m, c) {\n  eps = rnorm(m + 100)  \n  x = rep(NA, m + 100) \n  # On suppose pour notre condition initial\n  x[1] = eps[1] \n  \n  for (i in (2:(m + 100))) {\n    x[i] = eps[i] - c * x[i - 1]\n  }\n  \n  x_final = x[101:(m + 100)]\n  return(ts(x_final))  \n}\n```\n:::\n\n\n\n<br>\n\n#### QUESTION 2 : Pour $|c| = 0, .5, .9$, tracer une trajectoire simulée et sa suite des auto-corrélations empiriques\n<br>\n\n::: {.cell}\n\n```{.r .cell-code}\n# On pose nos paramètres\nm = 500\nc = c(-0.9, -0.5, 0, 0.5, 0.9)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 3 : Commenter les résultats\n<br>\nOn remarque qu'au moment où nos paramètres sont proche de $1$ ou $-1$, nos autocorrélations sont forte et notre série perd en stationnarité.\nEn effet, on remarque que le processus est un AR(1) avec son acf qui décroit exponentiellement et la stationnarité se perd quand $|c| \\longrightarrow 1$.\n\nOn remarque également que, qand $c=0$, on a un bruit blanc.\n\n\n\n\n<br>\n\n#### **BONUS**\n<br>\n\nEn Complément de ces informations, on peut aussi s'interesser au PACF \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-28-2.png){width=672}\n:::\n:::\n\n\nOn reconnait alors les caractéristiques d'un AR(1) au vu des ACF et PACF. Et le cas de $c=0$ apparait plus clairement comme celui d'un bruit blanc.\n<br>\n\n\n\n# **EXERCICE 4 : **\n<br>\n\n\nLe fichier champ.asc est disponible sur le web à l’adresse suivante\nhttp://www.math.sciences.univ-nantes.fr/~philippe/lecture/champ.asc\n\nOn note $(C_t)$ la série.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl = \"http://www.math.sciences.univ-nantes.fr/~philippe/lecture/champ.asc\"\ndata = read.csv(url)\n\nCt = ts(data)\n```\n:::\n\n<br>\n\n\n#### QUESTION 1 : Tracer la série $(C_t)$ et sa suite des auto-corrélations empiriques\n<br>\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 2 : En utilisant les résultats de l’exercice précédent, peut-on détecter la présence d’une fonction périodique ou d’une tendance dans cette série.\n<br>\nD'après les graphiques obtenus ainsi que l'analyse faite à l'exercice précédent, on constate clairement une série de type multplicatif (variance qui explose avec le temps) présentant une tendance linéaire et une saisonnalité (de période environ 12 peut-être).\n<br>\n\n#### QUESTION 3 : Tracer la série $(log(Ct))$ et sa suite des auto-corrélations empiriques\n<br>\n\n::: {.cell}\n\n```{.r .cell-code}\nlCt = log(Ct)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n\nOn peut observé que le passage au log à permis de contrer la croissance en $t$ de la variance.\n<br>\nEn effet, si on pose $Y_t = t\\varepsilon_t$, alors $Var(Y_t) = t^2Var(\\varepsilon_t)$.\n<br>\nOr, avec le passage au log, on aura que \n$Var(log(Y_t)) = Var(log(t\\varepsilon_t)) = Var(log(t)+log(\\varepsilon_t)) = Var(log(\\varepsilon_t))$\n\n\n<br>\n\n#### QUESTION 4 : Pour différentes valeurs des paramètres $(\\alpha, \\beta, \\gamma)$, simuler les séries suivantes de longueur 100 où $(\\varepsilon_t)$ est une suite de variables aléatoires i.i.d. $\\mathcal{N}_{(0, 1)}$\n<br>\nOn va donc simuler des séries de la forme suivante\n\\begin{align}\n\\alpha t + \\beta cos(\\frac{2πt}{12}) + \\gamma cos(\\frac{2πt}{6}) + \\beta' cos(\\frac{2πt}{12}) + \\gamma' cos(\\frac{2πt}{6}) + \\varepsilon_t\n\\end{align}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nut = function(n, a, b, c, d, e, a0 = 0){\n  t = 1:n\n  eps_t = rnorm(n)\n  \n  u = a0 + a*t + b*cos(pi/6*t)  + c*cos(pi/3*t) + d*sin(pi/6*t)  + e*sin(pi/3*t) + eps_t \n  return(u)\n}\n\n# Simulations pour différentes valeurs de coefficients\nn = rep(100,8)\nalpha = rep(c(0.01, 0.05),4)\nbeta = rep(c(-1,1,0.1,2),2)\ngamma = rep(c(-0.1,1,2,-0.5),2)\nd = gamma\ne = beta\na0 = rep(c(0, 7.5, 8, 8.5),2)\n\n\n# On stocke les simulations \nut.vect = Vectorize(ut)\nsimu.res = ut.vect(n,alpha,beta,gamma,d,e,a0)\n```\n:::\n\n\nOn peut déjà constater que cette série à été construite dans l'optique de prendre les périodes que l'on peut détecter avec l'ACF autour du Lag 6 et du Lag 12 et qui sont adéquate à l'aspect de \"double pics\" présent dans notre série. Le terme $\\alpha$ est de son côté, présent pour prendre en compte la présence de la tendance linéaire.\n\n<br>\n\n#### QUESTION 5 : Comparer l’allure des séries simulées avec la série des ventes de champagne et la série $(log(Ct))$.\n<br>\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n<br>\n\n#### QUESTION 6 : Pour laquelle des deux séries $((Ct))$ $(log(Ct))$, le modèle défini en question 4 vous semble le plus pertinent.\n<br>\n\nLa série $log(C_t)$ est la plus adaptée à l'halure des séries simulées car, pour les series simulées, on a pas la variance qui augmente comme pour$C_t$. \n<br>\n\n#### QUESTION 7 : Sur cette série, calculer les estimateurs de $(\\alpha, \\beta, \\gamma)$ par la méthode des moindres carrés. Que peut-on dire de la qualité du modèle. Peut on modéliser la série des résidus par un bruit blanc?\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#construisons un modèle de regression\nt = seq(1,length(Ct))\n\nmodel = lm(log(Ct) ~ t + cos((pi/6)*t)  + cos((pi/3)*t) + sin((pi/6)*t)  + sin((pi/3)*t))\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(Ct) ~ t + cos((pi/6) * t) + cos((pi/3) * t) + \n    sin((pi/6) * t) + sin((pi/3) * t))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76107 -0.13260  0.00618  0.18329  0.48198 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      8.1452854  0.0512649 158.886  < 2e-16 ***\nt                0.0043032  0.0008477   5.077 1.83e-06 ***\ncos((pi/6) * t)  0.2760137  0.0360024   7.667 1.30e-11 ***\ncos((pi/3) * t) -0.1698754  0.0361401  -4.700 8.49e-06 ***\nsin((pi/6) * t) -0.2165525  0.0360024  -6.015 3.11e-08 ***\nsin((pi/3) * t) -0.3990958  0.0357974 -11.149  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2592 on 98 degrees of freedom\nMultiple R-squared:  0.7243,\tAdjusted R-squared:  0.7103 \nF-statistic:  51.5 on 5 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nOn constate une forte significativité de tout nos termes.\n\n::: {.cell}\n::: {.cell-output-display}\n![](Exercice_01_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "Exercice_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}