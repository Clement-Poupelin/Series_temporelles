[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Séries temporelles enseignée dans le cadre du Master 2 IS à Nantes Université pour l’année universitaire 2023-2024.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#contexte",
    "href": "about.html#contexte",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Séries temporelles enseignée dans le cadre du Master 2 IS à Nantes Université pour l’année universitaire 2023-2024.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#objectif",
    "href": "about.html#objectif",
    "title": "About",
    "section": "Objectif",
    "text": "Objectif\nL’objectif de ce blog est de :\n\nFournir des analyses complètes et rigoureuses en réponse aux exercices abordés en travaux pratiques.\nProposer des solutions détaillées et commentées pour aider à la compréhension des méthodes statistiques.\nFavoriser une approche reproductible et bien documentée en utilisant R et Quarto."
  },
  {
    "objectID": "about.html#intervenant.e.s",
    "href": "about.html#intervenant.e.s",
    "title": "About",
    "section": "Intervenant.e.s",
    "text": "Intervenant.e.s\nLes rédacteurs et relecteurs des articles sont mentionnés sur chaque document du blog. Le travail collaboratif permet d’assurer la clarté, la rigueur et la qualité des contenus proposés."
  },
  {
    "objectID": "posts/Exercice_02.01.html",
    "href": "posts/Exercice_02.01.html",
    "title": "Exercice 2.01",
    "section": "",
    "text": "Clément Poupelin, clementjc.poupelin@gmail.com"
  },
  {
    "objectID": "posts/Exercice_02.01.html#plot-des-simulations",
    "href": "posts/Exercice_02.01.html#plot-des-simulations",
    "title": "Exercice 2.01",
    "section": "Plot des simulations",
    "text": "Plot des simulations\n\n\nShow the code\nplot_simulation_time_series &lt;- function(data, main_title, y_lab) {\n  plot_data &lt;- data.frame(\n    Time = rep(1:n, I),\n    Value = as.vector(data),\n    Group = rep(1:I, each = n)\n  )\n  \n  ggplot(plot_data, aes(\n    x = Time,\n    y = Value,\n    group = Group,\n    color = factor(Group)\n  )) +\n    geom_line() +\n    scale_color_viridis_d(name = \"Time séries\")  +\n    labs(title = main_title,\n    x = \"Time\",\n    y = y_lab) +\n    theme_minimal() + \n    theme(legend.title = element_text(size = 18),\n          legend.text = element_text(size = 12),\n          axis.title = element_text(size = 16),\n          axis.text = element_text(size = 14),\n          plot.title = element_text(size = 18, face = \"bold\"))\n  \n}"
  },
  {
    "objectID": "posts/Fiche_06.html",
    "href": "posts/Fiche_06.html",
    "title": "Fiche 06",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\nEXERCICE 1 : \n\nSur la série de vente de voitures (avant la rupture), montrer que la modélisation SARIMA ne fournit pas une solution satisfaisante. www.math.sciences.univ-nantes.fr/~philippe/lecture/voiture.txt \n\n\nShow the code\nurl_TP3 = \"http://www.math.sciences.univ-nantes.fr/~philippe/lecture/voiture.txt\"\nX = scan(url_TP3) # pour importer le dataframe en 1 vecteur de donnée (et non un dataframe de dim n*m)\n\nXt = ts(X, frequency =12)\n\nXt = window(Xt, start=start(Xt), end=24) # Avant rupture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn tente de trouver une modelisation sarima qui prend en compte la présence d’une tendance et une saisonnalité de période 12\n\n\nShow the code\nsarima_voitures = auto.arima(Xt, \n                             d=1, \n                             D=1,\n                             start.q = 1,\n                             start.p = 1,\n                             max.order = 4, \n                             stationary = FALSE,\n                             seasonal = TRUE, \n                             ic = \"bic\", \n                             trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n ARIMA(1,1,1)(1,1,1)[12]                    : 5555.015\n ARIMA(0,1,0)(0,1,0)[12]                    : 5729.47\n ARIMA(1,1,0)(1,1,0)[12]                    : 5658.897\n ARIMA(0,1,1)(0,1,1)[12]                    : 5531.315\n ARIMA(0,1,1)(0,1,0)[12]                    : 5629.095\n ARIMA(0,1,1)(1,1,1)[12]                    : 5549.975\n ARIMA(0,1,1)(0,1,2)[12]                    : 5536.018\n ARIMA(0,1,1)(1,1,0)[12]                    : 5603.51\n ARIMA(0,1,1)(1,1,2)[12]                    : 5545.522\n ARIMA(0,1,0)(0,1,1)[12]                    : 5636.085\n ARIMA(1,1,1)(0,1,1)[12]                    : 5537.866\n ARIMA(0,1,2)(0,1,1)[12]                    : 5536.856\n ARIMA(1,1,0)(0,1,1)[12]                    : 5592.743\n ARIMA(1,1,2)(0,1,1)[12]                    : 5540.004\n\n Now re-fitting the best model(s) without approximations...\n\n ARIMA(0,1,1)(0,1,1)[12]                    : 5795.21\n\n Best model: ARIMA(0,1,1)(0,1,1)[12]                    \n\n\n\n\nShow the code\nsummary(sarima_voitures)\n\n\nSeries: Xt \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.7189  -0.7981\ns.e.   0.0396   0.0496\n\nsigma^2 = 180095614:  log likelihood = -2889.24\nAIC=5784.48   AICc=5784.57   BIC=5795.21\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -264.0867 13051.56 9612.369 -1.658486 8.476118 0.6515145\n                    ACF1\nTraining set -0.01263232\n\n\nSi la modélisation choisie est valide, alors les résidus du modèle doivent former un bruit blanc. On peut alors regarder si l’ACF empirique et la PACF empirique des résidus sont similaires à celles d’un bruit blanc :\n\n\n\n\n\n\n\n\n\nOn peut utiliser la fonction coeftest de la librairie lmtest :\n\n\nShow the code\ncoeftest(sarima_voitures)\n\n\n\nz test of coefficients:\n\n      Estimate Std. Error z value  Pr(&gt;|z|)    \nma1  -0.718899   0.039585 -18.161 &lt; 2.2e-16 ***\nsma1 -0.798067   0.049578 -16.097 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nToutes las variables sont considérées comme significatives. Pourtant, les ACF et PACF ne seblent pas représenter un Bruit Blanc.\nEssayons quelques tests: \nTest de Ljung-Box : Utilisez le test de Ljung-Box pour tester l’hypothèse que les autocorrélations jusqu’à un certain nombre de retards (lag) sont nulles.\n\n\nShow the code\nBox.test(sarima_voitures[[\"residuals\"]], lag = 20, type = \"Ljung-Box\")\n\n\n\n    Box-Ljung test\n\ndata:  sarima_voitures[[\"residuals\"]]\nX-squared = 53.924, df = 20, p-value = 5.937e-05\n\n\nSi la p-value associée au test de Ljung-Box est significativement élevée, cela suggère que la série est un bruit blanc. Ici, ce n’est pas le cas.\n\nTest d’adéquation de Kolmogorov-Smirnov : Vous pouvez également utiliser le test d’adéquation de Kolmogorov-Smirnov pour comparer la distribution empirique de vos données avec une distribution normale.\n\n\nShow the code\nks.test(sarima_voitures[[\"residuals\"]], \"pnorm\")\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  sarima_voitures[[\"residuals\"]]\nD = 0.51976, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nSi la p-value est élevée, cela suggère que la série suit une distribution normale, ce qui est caractéristique d’un bruit blanc. Ici, ce n’est pas le cas. \n\nOn peut donc conclure que la modélisation SARIMA ne fournit pas une solution satisfaisante.\n\n\n\n\nEXERCICE 2 : \n\nOn souhaite prévoir le nombre d’usagers de la SNCF pour les 12 mois de l’année 2001. Pour réaliser la prévision, on dispose des données mensuelles sur 11 années entre 1990 et 2000. Ces données sont disponibles dans le fichier suivant www.math.sciences.univ-nantes.fr/~philippe/lecture/donnees-sncf-1990-2000.txt\n\n\nShow the code\nurl = \"http://www.math.sciences.univ-nantes.fr/~philippe/lecture/donnees-sncf-1990-2000.txt\"\ny = scan(url) # pour importer le dataframe en 1 vecteur de donnée (et ne pas avoir un truc de dim n*m)\nyt = ts(y, frequency = 12) # car données sur évolution annuelle \n\n\n\n\nQUESTION 1 : Justifier le choix d’une modélisation SARIMA sur cette série à l’aide de quelques graphiques. En déduire une estimation de \\((d, D, s)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCa ne ressemble pas à un MA ou un AR donc on peut tenter du SARIMA\n\n\nShow the code\ndy = diff(yt)\n\ntsplot(cbind(yt,dy), col = \"purple\", main = \"Comparaison yt et (I-B)yt\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\navec l’acf, on peut vouloir s=12, D=1, d=1\nRAPPEL :  Le modèle SARIMA d’ordres \\((p, d, q)(P, D, Q)_s\\) s’écrit de la forme\n\\[\\begin{align}\n  \\Phi(B^s)\\phi(B)(I-B^s)^D(I-B)^dX_t = \\Theta(B^s)\\theta(B)w_t\n\\end{align}\\]\nLes polynômes \\(\\phi\\) et \\(\\theta\\) représente la partie ARMA\nLes polynômes \\(\\Phi\\) et \\(\\Theta\\) représente la partie ARMA saisonnière\n\\((I-B^s)^D\\) et \\((I-B)^d\\) permettent de prendre en compte la non-stationnarité\nEn général \\(D=1\\) (ou \\(2\\))\n\n\n\nQUESTION 2 : Valider un ou plusieurs modèles SARIMA sur cette série\n\n\n\nShow the code\nmodele_sarima = auto.arima(yt, \n                           d=1, \n                           D=1,\n                           start.q = 1,\n                           start.p = 1, \n                           max.order = 4, \n                           stationary = FALSE, \n                           seasonal = TRUE, \n                           ic = \"bic\", \n                           trace = TRUE)\n\n\n\n ARIMA(1,1,1)(1,1,1)[12]                    : 1498.52\n ARIMA(0,1,0)(0,1,0)[12]                    : 1553.684\n ARIMA(1,1,0)(1,1,0)[12]                    : 1518.427\n ARIMA(0,1,1)(0,1,1)[12]                    : 1490.551\n ARIMA(0,1,1)(0,1,0)[12]                    : 1508.769\n ARIMA(0,1,1)(1,1,1)[12]                    : 1495.03\n ARIMA(0,1,1)(0,1,2)[12]                    : 1495.01\n ARIMA(0,1,1)(1,1,0)[12]                    : 1497.844\n ARIMA(0,1,1)(1,1,2)[12]                    : Inf\n ARIMA(0,1,0)(0,1,1)[12]                    : 1532.182\n ARIMA(1,1,1)(0,1,1)[12]                    : 1494.006\n ARIMA(0,1,2)(0,1,1)[12]                    : 1494.192\n ARIMA(1,1,0)(0,1,1)[12]                    : 1511.698\n ARIMA(1,1,2)(0,1,1)[12]                    : 1498.683\n\n Best model: ARIMA(0,1,1)(0,1,1)[12]                    \n\n\n\n\nShow the code\nsummary(modele_sarima)\n\n\nSeries: yt \nARIMA(0,1,1)(0,1,1)[12] \n\nCoefficients:\n          ma1     sma1\n      -0.7800  -0.5331\ns.e.   0.0725   0.0931\n\nsigma^2 = 13936:  log likelihood = -738.11\nAIC=1482.21   AICc=1482.42   BIC=1490.55\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 6.260106 111.1405 79.17467 0.1785064 3.036865 0.6068575 0.07031074\n\n\nDe manière non automatique\n\n\nShow the code\nmod1 = sarima(yt, p=1, d=1, q=1, P=0, D=1, Q=0, S=12)\n\n\n\n\n\n\n\n\n\nShow the code\nmod2 = sarima(yt, p=2, d=1, q=2, P=0, D=1, Q=0, S=12)\n\n\n\n\n\n\n\n\n\nShow the code\nmod3 = sarima(yt, p=3, d=1, q=3, P=0, D=1, Q=0, S=12)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmod1\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1\n      0.1185  -0.8458\ns.e.  0.1793   0.1446\n\nsigma^2 estimated as 17108:  log likelihood = -749.35,  aic = 1504.69\n\n$degrees_of_freedom\n[1] 117\n\n$ttable\n    Estimate     SE t.value p.value\nar1   0.1185 0.1793  0.6608    0.51\nma1  -0.8458 0.1446 -5.8478    0.00\n\n$ICs\n     AIC     AICc      BIC \n12.64448 12.64535 12.71454 \n\n\nShow the code\nmod2\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2      ma1      ma2\n      -0.3648  0.2793  -0.3614  -0.5584\ns.e.   0.2953  0.1081   0.2967   0.2614\n\nsigma^2 estimated as 16560:  log likelihood = -747.76,  aic = 1505.51\n\n$degrees_of_freedom\n[1] 115\n\n$ttable\n    Estimate     SE t.value p.value\nar1  -0.3648 0.2953 -1.2353  0.2193\nar2   0.2793 0.1081  2.5830  0.0110\nma1  -0.3614 0.2967 -1.2182  0.2257\nma2  -0.5584 0.2614 -2.1363  0.0348\n\n$ICs\n     AIC     AICc      BIC \n12.65137 12.65432 12.76814 \n\n\nShow the code\nmod3\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3      ma1     ma2      ma3\n      0.0264  -0.8078  -0.0497  -0.7504  0.8675  -0.7168\ns.e.  0.1662   0.1181   0.1801   0.1438  0.1391   0.1469\n\nsigma^2 estimated as 16187:  log likelihood = -746.27,  aic = 1506.55\n\n$degrees_of_freedom\n[1] 113\n\n$ttable\n    Estimate     SE t.value p.value\nar1   0.0264 0.1662  0.1587  0.8742\nar2  -0.8078 0.1181 -6.8423  0.0000\nar3  -0.0497 0.1801 -0.2758  0.7832\nma1  -0.7504 0.1438 -5.2182  0.0000\nma2   0.8675 0.1391  6.2371  0.0000\nma3  -0.7168 0.1469 -4.8812  0.0000\n\n$ICs\n     AIC     AICc      BIC \n12.66007 12.66637 12.82355 \n\n\nEn regadant un peu la significativité ainsi que les critères AIC, AICc et BIC, on pourrait vouloir tenter de retenir les modèles mod2 et mod3.\n\n\n\nQUESTION 3 : Pour les modèles SARIMA validés à la question précedente, calculer les prévisions mensuelles pour l’année 2001. Representer les differentes prévisions et les régions de confiance.\n\nRegardons pour nos modèles mod2 et mod3\n\n\nShow the code\npar(mfrow=c(2, 1)) \ns1 = sarima.for(yt, n.ahead = 12, p=2, d=1, q=2, P=0, D=1, Q=0, S=12) \ns2 = sarima.for(yt, n.ahead = 12, p=3, d=1, q=3, P=0, D=1, Q=0, S=12) \n\n\n\n\n\n\n\n\n\nMaintenant, si on prend le modèle choisi par auto.arima\n\n\nShow the code\ns3 = sarima.for(yt, n.ahead = 12, p=1, d=0, q=1, P=0, D=0, Q=2, S=12) \n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 4 : Evaluer la qualité de vos prévisions en les comparant avec les valeurs observées en 2001. Vous pouvez par exemple calculer les erreurs quadratiques\n\\(\\sum_{h=1}^{12}(\\hat{x}_{n:h} − x_{n+h})^2\\). \nLes sont disponibles dans le fichier https://www.math.sciences.univ-nantes.fr/~philippe/lecture/donnees-sncf-2001.txt\n\n\nShow the code\nurl_2001 = \"https://www.math.sciences.univ-nantes.fr/~philippe/lecture/donnees-sncf-2001.txt\"\ny_2001 = scan(url_2001) \nyt_2001 = ts(y_2001, frequency = 12, start = 12) \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\neq1=c()\nfor (i in 1:12){\n  eq1[i] = (s1$pre[1] - yt_2001[i])^2\n}\nEQ1 = sum(eq1)\n\neq2=c()\nfor (i in 1:12){\n  eq2[i] = (s2$pre[1] - yt_2001[i])^2\n}\nEQ2 = sum(eq2)\n\nEQ1\n\n\n[1] 2385213\n\n\nShow the code\nEQ2\n\n\n[1] 2364717\n\n\nShow the code\n# EQ2 est plus petite donc mod3 meilleur que mod2\n\n\navec le modèle de auto.arima\n\n\nShow the code\neq3=c()\nfor (i in 1:12){\n  eq3[i] = (s3$pre[1] - yt_2001[i])^2\n}\nEQ3 = sum(eq3)\n\nEQ1\n\n\n[1] 2385213\n\n\nShow the code\nEQ2\n\n\n[1] 2364717\n\n\nShow the code\nEQ3\n\n\n[1] 1706067\n\n\nIci, l’erreur est beaucoup plus petite donc le modèle semble meilleur\n\n\n\nQUESTION 5 : Comparer avec la qualité des prévision SARIMA avec celles données par la méthode de\nHolt Winter \n\n\nShow the code\nHW1 = HoltWinters(yt, start.periods = 2)\n\n#plot(yt, xlim=c(0,14))\n#lines(HW1$fitted[,1], lty=2, col=\"blue\")\n\nHW1.pred = predict(HW1, 12, prediction.interval = TRUE, level=0.95)\n#Visually evaluate the prediction\n#plot(yt, xlim=c(8,15), ylim = c(2500, 4000) )\nplot(yt_2001, col = \"black\", lwd=2, ylim = c(2600, 5000))\n#lines(HW1$fitted[,1], lty=2, col=\"blue\")\nlines(HW1.pred[,1], col=\"blue\", lty = 2)\n\npolygon(c(seq(12,13-1/12, by = 1/12), rev(seq(12,13-1/12, by = 1/12))), c(HW1.pred[,2], rev(HW1.pred[,3])), col=rgb(0, 0, 1,0.4), border = NA)\n# RGB (Rouge, Vert, Bleu), où (0, 0, 1)\n\nlines(s3$pred, col = \"red\", lty = 2)\nlegend(\"topleft\",\n       legend = c(\"yt_2001\", \"prédiction estimée\", \"prédictions HW\", \"intervalle de\\n prédictions HW\"),\n       col = c(\"black\", \"red\", \"blue\", rgb(0, 0, 1,0.4)),\n       lty = c(1, 2, 2, 1),\n       lwd = c(2, 1, 1, 12))\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 6 : Conclure"
  },
  {
    "objectID": "posts/Fiche_05.html",
    "href": "posts/Fiche_05.html",
    "title": "Fiche 05",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\nEXERCICE 1 : \n\nOn considère le modèle AR(2) défini par \\[\\begin{align}\nX_t + a_1X_{t-1} + a_2X_{t-2} = \\varepsilon_t\n\\end{align}\\]\noù \\((\\varepsilon_t)_t\\) est une suite iid suivant la loi standard gaussienne. \nPour les trois situations suivantes \\[\\begin{align}\n\na_1 = \\frac{-5}{6} & &  a_2 = \\frac{1}{6}\\\\\na_1 = \\frac{-5}{6} & & a_2 = 0.9\\\\\na_1 = -1.12 & & a_2 = 0.5\n\n\\end{align}\\]\n\n\nQUESTION 1 : Calculer les racines du polynôme AR\n\n\n\nShow the code\ncoefs_1 = c(-5/6, 1/6)\ncoefs_2 = c(-5/6, 0.9)\ncoefs_3 = c(-1.12, 0.5)\n\nracines_1 = polyroot(c(1,coefs_1))\nracines_2 = polyroot(c(1,coefs_2))\nracines_3 = polyroot(c(1,coefs_3))\n\n\n\n\n[1] \"racines_1 =  2+0i et 3+0i\"\n\n\n[1] \"racines_2 =  0.46+0.95i et 0.46-0.95i\"\n\n\n[1] \"racines_3 =  1.12+0.86i et 1.12-0.86i\"\n\n\n\n\n\nQUESTION 2 : Tracer la suite des ACF théoriques (ARMAacf)\n\n\n\nShow the code\ntrue_ACF_1 = ARMAacf(ar = (- coefs_1), lag.max = 30)\ntrue_ACF_2 = ARMAacf(ar = (- coefs_2), lag.max = 30)\ntrue_ACF_3 = ARMAacf(ar = (- coefs_3), lag.max = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 3 : Simuler et représenter une trajectoire de longueur \\(n = 2500\\)\n\n\n\nShow the code\nn = 2500\n\ntrajectoire_1 = arima.sim(list(ar = (-coefs_1)), n)\ntrajectoire_2 = arima.sim(list(ar = (-coefs_2)), n)\ntrajectoire_3 = arima.sim(list(ar = (-coefs_3)), n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 4 : A partir de la trajectoire simulée, comparer graphiquement les ACF théoriques et estimées\n\n\n\nShow the code\nlength(true_ACF_1)\n\n\n[1] 31\n\n\n\n\nShow the code\nacf(trajectoire_1)\nlines(0:30, true_ACF_1, type = \"h\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nacf(trajectoire_1,col = alpha(\"blue\", 0.5), lwd=2)\nlines(0:(length(true_ACF_1)-1), true_ACF_1, type = \"h\", col = alpha(\"red\", 0.5), lty = 2, lwd = 2)\nlegend(\"topright\",\n       legend = c(\"ACF empiriques\", \"ACF théoriques\"), \n       col = c(\"blue\",\"red\"),\n       lty = c(1,2))\n\n\n\n\n\n\n\n\n\nShow the code\nacf(trajectoire_2, col = alpha(\"blue\", 0.5), lwd=2)\nlines(0:(length(true_ACF_2)-1), true_ACF_2, type = \"h\", col = alpha(\"red\", 0.5), lty = 2, lwd = 2)\nlegend(\"topright\",\n       legend = c(\"ACF empiriques\", \"ACF théoriques\"), \n       col = c(\"blue\",\"red\"),\n       lty = c(1,2))\n\n\n\n\n\n\n\n\n\nShow the code\nacf(trajectoire_3, col = alpha(\"blue\", 0.5), lwd=2)\nlines(0:(length(true_ACF_3)-1), true_ACF_3, type = \"h\", col = alpha(\"red\", 0.5), lty = 2, lwd = 2)\nlegend(\"topright\",\n       legend = c(\"ACF empiriques\", \"ACF théoriques\"), \n       col = c(\"blue\",\"red\"),\n       lty = c(1,2))\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 5 : Représenter en fonction de \\(n\\) l’évolution des estimateurs de \\((a_1, a_2)\\)\n\nPour la première trajectoire\n\n\nShow the code\nn_values = seq(10, 2500, by = 10)\n\nfit_AR2 = function(data, n){\n  fit = arima(data[1:n], order = c(2,0,0), include.mean = FALSE, method = \"ML\")\n  #PB pour la trajetoire 2 car non inversible donc rajout method ML\n  return(c(-fit$coef[1], -fit$coef[2]))\n}\n\n\nmat1 = matrix(0, ncol = 2, nrow = length(n_values))\n\nfor (i in 1:length(n_values)){\n  mat1[i,] = fit_AR2(trajectoire_1, n_values[i])\n  \n}\n\n\n\n\n\n\n\n\n\n\n\nPour la deuxième trajectoire\n\n\nShow the code\nmat2 = matrix(0, ncol = 2, nrow = length(n_values))\n\nfor (i in 1:length(n_values)){\n  mat2[i,] = fit_AR2(trajectoire_2, n_values[i])\n}\n\n\n\n\n\n\n\n\n\n\n\nPour la troisième trajectoire\n\n\nShow the code\nmat3 = matrix(0, ncol = 2, nrow = length(n_values))\n\nfor (i in 1:length(n_values)){\n  mat3[i,] = fit_AR2(trajectoire_3, n_values[i])\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCICE 2 : Analyse de la série data(lynx) \n\nOn note la série \\((Ly_t)_t\\) . On cherche à modéliser les 109 premières valeurs de cette série par un processus stationnaire AR(p).  Les 5 dernières valeurs sont conservées pour évaluer les performances des prévisions réalisées.\n\n\nShow the code\n# Importation des données \ndata(lynx)\nLy = lynx[1:109]\n\n\n\n\nModèlisation\n\n\n\n\nQUESTION 1 : Tracer la série \\((Ly_t)\\), ses autocorrélations empiriques et ses autocorrélations partielles empiriques. Commenter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn peut reconnaitre un porcessus AR avec une acf qui décroit exponentiellement et une pacf qui a un cut off à partir du lag 8.\n\n\n\nQUESTION 2 : Modéliser cette série par un processus AR d’ordre \\(p\\)\n\nOn va donc modéliser plusieurs processus avec au maximum \\(p=8\\). Pour cela on va utiliser la fonction auto.arima\n\n\nShow the code\nmodele_ar = auto.arima(ts(Ly),d=0,D=0,max.p = 8, max.q = 0, max.order = 8, start.p = 8, stationary = TRUE, seasonal = FALSE, ic = \"bic\", trace = TRUE)\n\n\n\n ARIMA(8,0,0) with non-zero mean : 1816.864\n ARIMA(0,0,0) with non-zero mean : 1926.811\n ARIMA(1,0,0) with non-zero mean : 1853.706\n ARIMA(0,0,0) with zero mean     : 1992.563\n ARIMA(7,0,0) with non-zero mean : 1823.973\n ARIMA(8,0,0) with zero mean     : 1821.505\n\n Best model: ARIMA(8,0,0) with non-zero mean \n\n\n\n\nShow the code\nsummary(modele_ar)\n\n\nSeries: ts(Ly) \nARIMA(8,0,0) with non-zero mean \n\nCoefficients:\n         ar1      ar2     ar3      ar4      ar5     ar6      ar7     ar8\n      1.0531  -0.6273  0.2099  -0.1429  -0.0198  0.0365  -0.2363  0.3298\ns.e.  0.0900   0.1336  0.1461   0.1460   0.1465  0.1455   0.1329  0.0924\n           mean\n      1549.7982\ns.e.   189.4505\n\nsigma^2 = 700446:  log likelihood = -884.98\nAIC=1789.95   AICc=1792.2   BIC=1816.86\n\nTraining set error measures:\n                    ME   RMSE     MAE       MPE     MAPE      MASE        ACF1\nTraining set -15.39004 801.63 558.127 -45.86168 114.6477 0.6625672 -0.02139499\n\n\n\n\n\nQUESTION 3 : Peut on valider la modélisation obtenue\n\nSi la modélisation choisie est valide, alors les résidus du modèle doivent former un bruit blanc.  On peut alors regarder si l’ACF empirique et la PACF empirique des résidus sont similaires à celles d’un bruit blanc :\n\n\nShow the code\npar(mfrow=c(1, 2))\nacf(modele_ar[[\"residuals\"]])\npacf(modele_ar[[\"residuals\"]])\n\n\n\n\n\n\n\n\n\nShow the code\npar(mfrow=c(1, 1))\n\n\nLes autocorrélations empiriques suggèrent donc que les résidus forment un bruit blanc.\nIl est néanmoins important de vérifier si les paramètres du modèle sont significatifs (surtout le coefficient d’ordre p !!!).\nOn peut le faire (manuellement) par un test asymptotique de la significativité individuelle des coefficients en comparant par exemple les coefficients standardisés aux quantiles de la loi normale standard (\\(\\pm 1.96\\) pour un test bilatéral au risque \\(\\alpha = 5\\%\\)) :\n\n\nShow the code\nabs(modele_ar[[\"coef\"]] / sqrt(diag(modele_ar[[\"var.coef\"]]))) &gt;= 1.96\n\n\n      ar1       ar2       ar3       ar4       ar5       ar6       ar7       ar8 \n     TRUE      TRUE     FALSE     FALSE     FALSE     FALSE     FALSE      TRUE \nintercept \n     TRUE \n\n\nIl est également possible d’utiliser la fonction coeftest de la librairie lmtest :\n\n\nShow the code\ncoeftest(modele_ar)\n\n\n\nz test of coefficients:\n\n             Estimate  Std. Error z value  Pr(&gt;|z|)    \nar1          1.053061    0.090009 11.6995 &lt; 2.2e-16 ***\nar2         -0.627343    0.133628 -4.6947 2.670e-06 ***\nar3          0.209905    0.146086  1.4369 0.1507583    \nar4         -0.142866    0.145959 -0.9788 0.3276746    \nar5         -0.019799    0.146459 -0.1352 0.8924675    \nar6          0.036463    0.145479  0.2506 0.8020931    \nar7         -0.236312    0.132936 -1.7776 0.0754641 .  \nar8          0.329843    0.092361  3.5712 0.0003553 ***\nintercept 1549.798224  189.450452  8.1805 2.827e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn peut alors ré-estimer le modèle en fixant les coefficients non significatifs à 0 (ce qui revient à supprimer les variables non significatives dans la régression linéaire multiple : cette étape fait donc partie de la sélection du modèle et non de la validation proprement-dite)\n\n\nShow the code\nmodele_ar_final = Arima(Ly, order = c(8,0,0), include.mean = TRUE, method = c(\"CSS-ML\", \"ML\", \"CSS\"),\n  fixed = c(NA, NA, 0, 0, 0, 0, 0, NA, NA))\n\n\n\n\nShow the code\nsummary(modele_ar_final)\n\n\nSeries: Ly \nARIMA(8,0,0) with non-zero mean \n\nCoefficients:\n         ar1      ar2  ar3  ar4  ar5  ar6  ar7     ar8       mean\n      1.0854  -0.4857    0    0    0    0    0  0.1836  1545.0887\ns.e.  0.0737   0.0788    0    0    0    0    0  0.0572   353.1573\n\nsigma^2 = 750280:  log likelihood = -891.11\nAIC=1792.21   AICc=1792.8   BIC=1805.67\n\nTraining set error measures:\n                    ME    RMSE      MAE       MPE     MAPE      MASE\nTraining set -4.378324 850.145 583.7302 -54.96815 134.3959 0.6929614\n                     ACF1\nTraining set -0.006081288\n\n\n\n\n\n\n\n\n\n\n\nLes ACF et PACF empiriques des résidus du modèle correspondent à ceux d’un bruit blanc. On peut donc “a priori” valider ce modèle. \n (Il faut noter quand même que la variance des résidus sigma^2 est très élevée !!! Une transformation log pourrait être intéressante si nécessaire)\n\n\n\nQUESTION 4 : Calculer et représenter les racines du polynôme auto-régressif\n\n\n\nShow the code\n# On prend le modèle AR avec coef non significatifs\nmodele_ar_coef = modele_ar[[\"coef\"]][paste0('ar',1:8)] \n\n\n\n\nShow the code\n# On calcul les racines \nracines_poly_ar = polyroot(c(1, -modele_ar_coef))\n\n\n\n\n[1]  0.83+0.63i -0.66+0.99i -0.66-0.99i  0.83-0.63i  0.24+1.10i -1.29+0.00i\n[7]  0.24-1.10i  1.19+0.00i\n\n\n[1] \" \"\n\n\n[1] \"Module de la 1 ière racine :  1.04\" \"Module de la 2 ième racine :  1.2\" \n[3] \"Module de la 3 ième racine :  1.2\"  \"Module de la 4 ième racine :  1.04\"\n[5] \"Module de la 5 ième racine :  1.13\" \"Module de la 6 ième racine :  1.29\"\n[7] \"Module de la 7 ième racine :  1.13\" \"Module de la 8 ième racine :  1.19\"\n\n\n\n\nShow the code\n# On prend le modèle AR avec coef non significatifs fixés à 0\nmodele_ar_final_coef = modele_ar_final[[\"coef\"]][paste0('ar',1:8)] \n\n\n\n\nShow the code\n# On calcul les racines\nracines_poly_ar_final = polyroot(c(1, -modele_ar_final_coef))\n\n\n\n\n[1]  0.82+0.68i -0.88+1.09i  0.24-1.22i  0.82-0.68i  0.24+1.22i -1.45+0.00i\n[7] -0.88-1.09i  1.10+0.00i\n\n\n[1] \" \"\n\n\n[1] \"Module de la 1 ière racine :  1.06\" \"Module de la 2 ième racine :  1.4\" \n[3] \"Module de la 3 ième racine :  1.24\" \"Module de la 4 ième racine :  1.06\"\n[5] \"Module de la 5 ième racine :  1.24\" \"Module de la 6 ième racine :  1.45\"\n[7] \"Module de la 7 ième racine :  1.4\"  \"Module de la 8 ième racine :  1.1\" \n\n\nToutes les racines sont en dehors du disque unité : le processus estimé est causal\nOn peut les représenter et voir leurs positions par rapport rapport au cercle unité :\n\n\nShow the code\n# racines\ncm = 1/2.54 # conversion centimètres en pouces\nxlim = range(c(-1, 1, Re(racines_poly_ar_final)))\nylim = range(c(-1, 1, Im(racines_poly_ar_final)))\nysize = par(\"fin\")[2]\n\n\npar(fin=c(ysize, ysize))\nplot(racines_poly_ar_final, pch = \"*\", cex = 2, col = \"red\",\n  xlim = xlim, \n  ylim = ylim,\n  main = \"Racines du polynôme AR et cercle unité\",\n  xlab = \"axe réel\", ylab = \"axe imaginaire\")\n\n# ajout du cercle unité\nx = seq(-1,1, length.out = 100)\ny = c(sqrt(1 - x^2), -sqrt(1 - rev(x)^2))\nlines(c(x,rev(x)), y, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\nComparaison avec une série simulée suivant le modèle estimé\n\n\n\n\n\nQUESTION 5 : Simuler une trajectoire de longueur 109 suivant le modèle autorégressif obtenu à la question 2)\n\nOn simule en tenant compte de la variance des résidus et du fait que le processus estimé est non centré (on sait que intercept = mu * (1 - sum(coef(AR))), où mu désigne la moyenne du processus) :\n\n\nShow the code\n# On pose nos paramètres\nsd = sqrt(modele_ar_final$sigma2)\nmu = modele_ar_final[[\"coef\"]][[\"intercept\"]]/sum(c(1, -modele_ar_final_coef))\n\n\ntrajectoire = mu + arima.sim(list(ar = modele_ar_final$model$phi), 109, sd = sd)\n\ntrajectoire\n\n\nTime Series:\nStart = 1 \nEnd = 109 \nFrequency = 1 \n  [1] 6296.894 6679.557 7517.424 7480.679 6551.314 6147.638 6692.894 8395.757\n  [9] 7950.002 6221.558 6036.222 6007.872 6154.923 6507.039 6827.196 7304.711\n [17] 6566.729 5385.287 4724.883 5578.211 5501.938 4945.395 4391.066 5026.821\n [25] 5716.665 7523.909 8203.676 6975.930 5543.028 3384.652 2452.578 1139.244\n [33] 2243.897 5343.136 7261.971 8794.179 8299.870 7186.059 6088.399 4111.452\n [41] 3576.965 3331.020 3059.353 4794.482 8082.696 9301.217 8753.599 6960.391\n [49] 5312.231 4005.593 4637.021 4772.347 6730.478 6843.427 7608.774 7902.327\n [57] 5599.462 4734.936 5838.209 6528.383 6841.928 6962.252 6262.182 5794.706\n [65] 6338.485 6879.422 5537.375 4671.717 5274.061 5336.618 6076.776 6466.981\n [73] 7036.544 6730.531 7128.128 8037.532 7304.329 5632.829 4549.681 3936.662\n [81] 5945.391 6912.281 6945.500 7219.632 7198.676 7022.255 6249.562 6297.565\n [89] 6258.140 6865.752 7952.480 9147.047 7776.166 6097.659 4892.835 5424.848\n [97] 6526.786 8240.159 9591.384 9882.140 8663.513 7671.786 7887.016 8765.456\n[105] 8309.337 7793.462 7231.463 7266.603 7440.669\n\n\nOn peut déjà remarquer que cette série simulée prend de plus grandes valeurs par rapport aux valeurs prises par rapport à la série lynx. Ce constat peut s’expliquer entre autres par la différence qu’il peut y avoir entre la moyenne théorique mu estimée ci-dessus pour un processus AR(p) stationnaire avec la moyenne théorique d’un processus présentant une tendance saisonnière similaire à celle du processus sous-jacent aux données lynx.\n\n\n\nQUESTION 6 : Tracer la série simulée, ses autocorrélations empiriques et ses autocorrélations partielles empiriques. Commenter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA part la taille des valeurs prises par la série simulée déjà évoquées, on voit aussi que l’aspect global de la série n’est pas le même que celui de la série lynx.\nEn effet, là où la série lynx semble présenter une périodicité d’environ 10 ans, il n’en est pas de même de la série simulée (ce qui est tout à fait normal pour un processus AR). \nSon ACF empirique présente une pseudo-période de 8 ans (due aux racines complexes du polynôme AR) et son PACF empirique laisserait penser que c’est un processus AR d’ordre \\(p &lt; 8\\) et non \\(8\\) (tout dépend de l’aléa dans les différentes simulations : les résultats ne seront pas forcément les mêmes d’une simulation à une autre, sauf si la graine aléatoire est fixée au préalable), un phénomène probablement dû au nombre faible de données (\\(109\\) ici) ne permettant pas des estimations assez consistantes des autocorrélations d’ordre supérieur dans ce cas précis (en simulant une série de taille largement plus grande que 109, on obtiendrait le comportement attendu pour la PACF empirique).\n Néanmoins, ce constat est un signal d’alerte pour faire attention aux estimations obtenues pour la série lynx : \\(109\\) ce n’est peut-être pas assez et il faut donc être vigilant quant aux conclusions inférées à partir de la modélisation faite.\n\n\nPrévision\n\n\n\n\n\nQUESTION 7 : À partir du modèle estimé, calculer les prévisions \\(\\hat{L}y_{110}, · · · , \\hat{L}y_{114}\\). Représenter sur un même graphique les prévisions, les valeurs de la série et l’intervalle de prévision\n\nOn peut regarder ce que l’on obtient avec le modèle AR estimé précédemment en terme des performances prédictives en utilisant les dernières valeurs de la série non incluse dans la modélisation\n\n\nShow the code\npred = forecast(modele_ar_final, h=5)\nprint(pred)\n\n\n    Point Forecast     Lo 80    Hi 80      Lo 95    Hi 95\n110       677.5229 -432.5403 1787.586 -1020.1723 2375.218\n111      1042.4638 -595.7932 2680.721 -1463.0340 3547.962\n112      1583.7922 -225.7879 3393.372 -1183.7217 4351.306\n113      2203.8110  377.1790 4030.443  -589.7815 4997.403\n114      2496.5023  666.9676 4326.037  -301.5295 5294.534\n\n\nShow the code\nLy_test = lynx[110:114]\nprint(Ly_test)\n\n\n[1]  662 1000 1590 2657 3396\n\n\nOn peut calculer l’erreur moyenne quadratique des prédictions à l’horizon \\(h = 5\\) :\n\n\nShow the code\nMSE_pred = sum((pred$mean - Ly_test)^2)/length(pred$mean)\nprint(MSE_pred)\n\n\n[1] 203311.8\n\n\nLa MSE en prédiction est énorme. En regardant les prédictions ponctuelles de près, c’est surtout les deux dernières valeurs qui font exploser la MSE :\n\n\nShow the code\nprint(pred$mean - Ly_test)\n\n\nTime Series:\nStart = 110 \nEnd = 114 \nFrequency = 1 \n[1]   15.522856   42.463798   -6.207789 -453.189028 -899.497699\n\n\nN.B. Si on avait conservé le modèle avec plusieurs coefficients non significatifs, les performances prédictives auraient été encore pires avec une MSE davantage grande. \nOn peut ensuite représenter sur un même graphique les prévisions, les valeurs de la série et l’intervalle de prévision (et pourquoi pas les valeurs passées de la série telles que estimées par le modèle lors de son ajustement : ce dernier point est à faire lors de la validation du modèle normalement).\nLa fonction autoplot de la librairie ggplot2 fait presque tout le travail en une ligne de code si on lui passé un objet produit par la fonction forecast de la librairie du même nom :\n\n\nShow the code\nautoplot(pred, main=\"Prédictions de cinq dernières valeurs de la série lynx\", ylab='Nombre de lynx', xlab='Années') + theme_light()\n\n\n\n\n\n\n\n\n\nSinon, on peut aussi faire les choses “à la main” :\n\n\n\n\n\n\n\n\n\nPour conclure, nous pouvons voir que ce modèle est potentiellement adapté à des fins de prévision à très court terme (horizons \\(h = 2\\) ans ou \\(h =3\\) ans).\nEn effet, même si il s’ajuste un peu bien sur ces données lynx (cf. la courbe fitted values ci-dessus) et semble suivre le phénomène périodique de la série, nous savons qu’il est non périodique (AR(8)) et nous avons vu précédemment qu’une série simulée suivant ce modèle ne présentait pas les mêmes structures macroscopiques.\nDe plus, en simulant une série plus longue, on perd de plus en plus les détails périodiques. D’où les prévisions à long terme qui s’éloigneront davantage de la vraie tendance (saisonnière) sous-jacente au processus générant les données.\nEnfin, il est impossible de ne pas noter la surdispersion des intervalles de prévision (les bornes inférieures à 95% et 80% donnent des nombres de lynx négatifs !!!).\n\n\n\n\n\nEXERCICE 3 : Analyse de la série varve \n\nLa série varve, disponible dans la librarie astsa, contient l’enregistrement des dépots sédimentaires (varve glacière) dans le Massachusetts pendant 634 années ( il y a près de 12 000 ans). La série (notée \\(x_t\\)) montre une certaine non-stationnalité.\n\n\nQUESTION 1 : Comparer la variance de l’échantillon sur la première moitié et la seconde moitié des données. Commenter\n\n\n\nShow the code\ndygraph(varve, main = \"Trajectoire de la série varve\",  ylab = \"X_t\", xlab='temps') %&gt;% \n  dyRangeSelector()\n\n\n\n\n\n\n\n\nShow the code\nvar_moitie_1 = var(varve[1:317])\nvar_moitie_2 = var(varve[318:634])\n\n\n\n\n[1] \"Variance de la première moitié :  133.457415667053\"\n\n\n[1] \"Variance de la seconde moitié :  594.490438823224\"\n\n\n[1] \"Comparaison v2/v1 :  4.45453282496005\"\n\n\nLa variance de la seconde moitié de données vaut plus de quatre fois celle de la première moitié. La variance n’est donc pas constante et la série est par conséquent non stationnaire.\n\n\n\nQUESTION 2 : On applique la transformation \\(y_t = log(x_t)\\). Illustrer que cette transformation stabilise la variance de la série. Représenter l’évolution de la variance empirique calculée sur des blocs de longueur \\(m\\). (utiliser rollapply de la librairie zoo )\n\n\n\nShow the code\n# Transformation log \nyt = log(varve)\n\n\n\n\n\n\n\n\nOn compare à nouveau les variances\n\n\nShow the code\nprint(paste(\"Variance de la première moitié : \", var(yt[1:317])))\n\n\n[1] \"Variance de la première moitié :  0.270721652653357\"\n\n\nShow the code\nprint(paste(\"Variance de la seconde moitié : \", var(yt[318:634])))\n\n\n[1] \"Variance de la seconde moitié :  0.451371011716303\"\n\n\nShow the code\nprint(paste(\"Comparaison v2/v1 : \", var(yt[318:634])/var(yt[1:317])))\n\n\n[1] \"Comparaison v2/v1 :  1.6672881806549\"\n\n\n\n\nShow the code\nm = 30\nevol_var = rollapplyr(yt, width = m, FUN = var)\n\n\ndygraph(evol_var, main = paste0(\"Evolution variance empirique Blocs de longueur m = \", m),\n        ylab = \"Variance empirique\",\n        xlab='Blocs') %&gt;% \n  dyRangeSelector()\n\n\n\n\n\n\nLa variance se stabilise en moyenne autour de 0.2 pour des blocs de longueur \\(m=30\\)\n\n\n\nQUESTION 3 : Tracer les histogrammes de \\(x_t\\) et \\(y_t\\). Commenter l’effet de la transformation log sur la loi.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa transformation log rend la loi de \\(y_t\\) moins asymétrique que celle de \\(x_t\\)\n\n\n\nQUESTION 4 : Représenter les autocorrélations de \\(y_t\\). Commenter\n\n\n\n\n\n\n\n\n\n\nL’ACF empirique suggère la présence d’une tendance polynomiale dans la dynamique de \\(y_t\\).\n\n\n\nQUESTION 5 : Calculer \\(u_t = y_t − y_{t−1}\\) et analyser les propriétés de cette séries. La différenciation des données \\(y_t\\) produit elle une série raisonnablement stationnaire ?\n\n\n\nShow the code\nut = diff(yt)\n\n\n\n\n\n\n\n\nVisuellement, la série semble stationnaire. \n\n\nShow the code\nadf.test(ut)\n\n\nWarning in adf.test(ut): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ut\nDickey-Fuller = -11.824, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nShow the code\nkpss.test(ut)\n\n\nWarning in kpss.test(ut): p-value greater than printed p-value\n\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ut\nKPSS Level = 0.012671, Truncation lag parameter = 6, p-value = 0.1\n\n\nEn complétant l’analyse visuelle avec des tests de stationnarité au risque \\(\\alpha = 5\\%\\), le test ADF rejette la non stationnarité et le test KPSS ne rejette pas la stationnarité.\n\n\n\nQUESTION 6 : Représenter les autocorrélations empiriques et les autocorrélations partielles empiriques de la série \\((u_t)\\) Le modèle MA(1) vous semble-t-il justifié pour modéliser la série \\((u_t)\\) ?\n\n\n\n\n\n\n\n\n\n\nL’ACF empirique suggère que la modélisation MA(1) comme une probable modélisation adéquate sur la série différenciée (et la PACF empirique présente une décroissante exponentielle en valeur absolue avec toutefois des pics atypiques)\n\n\n\nQUESTION 7 : Calculer une estimation des paramètres du modèe ARIMA(0,1,1) sur la série \\((y_t)\\)\n\n\n\nShow the code\nmodele_0_1_1 = Arima(yt, order = c(0,1,1))\nsummary(modele_0_1_1)\n\n\nSeries: yt \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7705\ns.e.   0.0341\n\nsigma^2 = 0.2357:  log likelihood = -440.72\nAIC=885.44   AICc=885.45   BIC=894.34\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.004996908 0.4847107 0.3816333 -2.672511 13.23327 0.842296\n                  ACF1\nTraining set 0.1196371\n\n\n\n\n\nQUESTION 8 : Peut on valider le modèle estimé sur la série \\((y_t)\\)\n\n\n\nShow the code\ncoeftest(modele_0_1_1)\n\n\n\nz test of coefficients:\n\n    Estimate Std. Error z value  Pr(&gt;|z|)    \nma1 -0.77054    0.03407 -22.616 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLe coefficient du modèle est significatif. On peut passer à l’analyse des résidus :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nBox.test(modele_0_1_1$residuals, lag = 2, fitdf = 1)\n\n\n\n    Box-Pierce test\n\ndata:  modele_0_1_1$residuals\nX-squared = 9.4803, df = 1, p-value = 0.002077\n\n\nLes ACF et PACF empiriques de résidus du modèle ne correspondent pas à celles d’un bruit blanc ; ce qui est confirmé par le test de portemanteau (ici Box-Pierce) qui rejette l’hypothèse nulle de blancheur (indépendance) des résidus. On ne peut donc pas valider ce modèle.\nSi les résidus formaient un bruit blanc, on aurait alors pu vérifier la normalité, hypothèse utilisée souvent pour le calcul des intervalles de prédiction :\n\n\nShow the code\nhist(modele_0_1_1$residuals, freq = FALSE, col = \"skyblue\")\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nshapiro.test(modele_0_1_1$residuals)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  modele_0_1_1$residuals\nW = 0.99575, p-value = 0.08266\n\n\nOn aurait pu conclure que les résidus sont approximativement gaussiens (histogramme grossièrement semblable à celui d’une gaussienne et le test de Shapiro Wilk ne rejette pas l’hypothèse de normalité au risque 5%)\n\n\n\nQUESTION 9 : Si ce modèle n’est pas validé, proposer une autre modélisation ARIMA pour la série \\((y_t)\\)\n\nOn pourrait explorer différents modèles différenciés proches de ARIMA(0,1,1) (par exemple ARIMA(1,1,1) avec et sans drift, etc) et garder celui qui donne des résultats satisfaisants. On peut aussi s’aider de la fonction auto.arima pour faire ces tests de manière automatisée et garder le meilleur modèle suivant le critère d’information de son choix (BIC, AIC, etc)\n\n\nShow the code\nauto.arima(yt, d=1, stationary = FALSE, seasonal = FALSE, ic = \"bic\", trace = TRUE)\n\n\n\n Fitting models using approximations to speed things up...\n\n ARIMA(2,1,2) with drift         : 903.2646\n ARIMA(0,1,0) with drift         : 1111.827\n ARIMA(1,1,0) with drift         : 1010.402\n ARIMA(0,1,1) with drift         : 901.1994\n ARIMA(0,1,0)                    : 1105.379\n ARIMA(1,1,1) with drift         : 889.4259\n ARIMA(2,1,1) with drift         : 898.7784\n ARIMA(1,1,2) with drift         : 894.7764\n ARIMA(0,1,2) with drift         : 891.4283\n ARIMA(2,1,0) with drift         : 979.2122\n ARIMA(1,1,1)                    : 883.1899\n ARIMA(0,1,1)                    : 894.8155\n ARIMA(1,1,0)                    : 1003.956\n ARIMA(2,1,1)                    : 892.6602\n ARIMA(1,1,2)                    : 888.607\n ARIMA(0,1,2)                    : 885.1202\n ARIMA(2,1,0)                    : 972.7741\n ARIMA(2,1,2)                    : 897.1243\n\n Now re-fitting the best model(s) without approximations...\n\n ARIMA(1,1,1)                    : 882.2265\n\n Best model: ARIMA(1,1,1)                    \n\n\nSeries: yt \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.2330  -0.8858\ns.e.  0.0518   0.0292\n\nsigma^2 = 0.2292:  log likelihood = -431.44\nAIC=868.88   AICc=868.91   BIC=882.23\n\n\nOn va ajuster le modèle ARIMA(1,1,1) sans drift (car meilleur modèle au sens du BIC).\nOn remarque tout de même que notre série non différenciée en possède. On pourrait donc également tester un modèle avec drift.\n\n\nShow the code\nmodele_arima = Arima(yt, order = c(1,1,1), include.drift = FALSE)\nsummary(modele_arima)\n\n\nSeries: yt \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.2330  -0.8858\ns.e.  0.0518   0.0292\n\nsigma^2 = 0.2292:  log likelihood = -431.44\nAIC=868.88   AICc=868.91   BIC=882.23\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.008779553 0.4775705 0.3754852 -2.824298 13.03097 0.8287266\n                     ACF1\nTraining set -0.008706557\n\n\n\n\nShow the code\ncoeftest(modele_arima)\n\n\n\nz test of coefficients:\n\n     Estimate Std. Error  z value  Pr(&gt;|z|)    \nar1  0.233002   0.051785   4.4994 6.813e-06 ***\nma1 -0.885763   0.029150 -30.3861 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn étudit les résidus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nBox.test(modele_arima$residuals, lag = 3, fitdf = 2)\n\n\n\n    Box-Pierce test\n\ndata:  modele_arima$residuals\nX-squared = 1.0878, df = 1, p-value = 0.297\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nshapiro.test(modele_arima$residuals)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  modele_arima$residuals\nW = 0.99752, p-value = 0.4681\n\n\nLes résidus satisfont les différents tests non satisfaits précédemment (blancheur) et sont approximativement gaussiens\n\n\n\nQUESTION 10 : Pour la modélisation que vous avez retenue (et donc validée ) calculer la prévision aux horizons 1 à 20 avec des intervalles de prévision, pour la série \\((y_t)\\) puis la série initiale varve\n\nOn utilise donc le modèle ARMA(1,1,1) sans drift pour prévoir \\(y_t\\) et ensuite \\(x_t\\)\n\n\nShow the code\npred_yt = forecast(modele_arima, h = 20)\n\n\n\n\n\n\n\n\n\n\n\nPour les prédictions de \\(x_t\\), il ne faut pas oublier le facteur multiplicatif de correction induit par le passage du log à exp :\n\n\nShow the code\npred = pred_yt\npred$mean = exp(pred$mean + pred$model$sigma2/2)\npred$lower = exp(pred$lower)\npred$upper = exp(pred$upper)\npred$fitted = exp(pred$fitted + pred$residuals)\npred$x = exp(pred$x)\n\n\n\n\n\n\n\n\n\n\n\nOn fait une visualisation plus détaillée\n\n\n\n\n\n\n\n\n\nLes prévisions sont similaires à celles d’un modèle MA(1) pas adapté à de la prévision à un horizon lointain."
  },
  {
    "objectID": "posts/Exercice_03.01.html",
    "href": "posts/Exercice_03.01.html",
    "title": "Exercie 3.1",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\nlibrary(latex2exp)\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\nTendance et saisonnalité : étude de la série de vente de voitures. http://www.math.sciences.univ-nantes.fr/~philippe/lecture/voiture.txt\nOn conserve uniquement le début de la série avant la rupture.\n\n\nShow the code\nurl_TP3 = \"http://www.math.sciences.univ-nantes.fr/~philippe/lecture/voiture.txt\"\nX = scan(url_TP3) # pour importer le dataframe en 1 vecteur de donnée (et non un dataframe de dim n*m)\n\nXt = ts(X, frequency =12)\n# On fixe une fréquence de 12 \n\n\n\n\n\n\n\n\n\n\n\nOn constate une rupture au niveau du temps 24. On ne conserve donc que la première partie pour notre analyse.\n\n\nShow the code\nXt = window(Xt, start=start(Xt), end=24)\n\n\nQUESTION 1 : A l’aide de la suite des covariances empiriques, mettre en évidence la présence d’une tendance et d’une composante saisonnière dans cette série\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn constate effectivement une tendance globale sur toute la série au vu de nos deux graphes. De plus, l’ACF nous permet de voir une composante saisonnière qui serait probablement de période 12.\n\n\nAnayse inférentielle\n\nApproche 1Approche 2\n\n\nQUESTION 2 : Estimer la tendance et la composante saisonnière de cette série\nPour cette partie, on peut se référé au cours de Séries Temporelle (slides 37-51).\n\n\nShow the code\n#### ETAPE 1 : LISSAGE/FILTRAGE ####\n\n# on suppose période 12 d'après l'acf\nd = 12\n# si période d pair, filtre :\na = c(1/2,rep(1,d-1),1/2)/d\n\nm = stats::filter(Xt, a, sides = 2)\nZ = Xt - m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe lissage d’une série chronologique (ou le filtrage) est utilisé pour découvrir certaines caractéristiques d’une série temporelle, telles que la tendance et les composantes saisonnières.\n\n\nShow the code\n#### ETAPE 2 : ELIMINATION SAISONNALITE ET TENDANCE ####\n\n# Saisonnalité\nZ = ts(Z, frequency = d) # impose la fréquence/période\ns.est = 1:12\nfor ( i in 1:12 ){\n  s.est[i]= mean(Z[cycle(Z) == i] , na.rm=TRUE)\n}\n\n\nS.est = ts(rep(s.est,d), frequency = frequency(Xt)) # Saisonnalité estimée\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Tendance \n\nQ = Xt - s.est \n\n\nWarning in `-.default`(Xt, s.est): la taille d'un objet plus long n'est pas\nmultiple de la taille d'un objet plus court\n\n\nShow the code\nmod = lm(Q ~ time(Q)) # time(Q) ca fait 1:lenght(Q)\nsummary(mod)\n\n\n\nCall:\nlm(formula = Q ~ time(Q))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-51203 -10213   -318   9711  36567 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  57964.5     1882.0   30.80   &lt;2e-16 ***\ntime(Q)       4958.3      132.9   37.32   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14730 on 275 degrees of freedom\nMultiple R-squared:  0.8351,    Adjusted R-squared:  0.8345 \nF-statistic:  1393 on 1 and 275 DF,  p-value: &lt; 2.2e-16\n\n\nShow the code\nP.est = ts(mod$fitted.values, frequency = frequency(Xt)) # Tendance estimée\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 3 : Éliminer la tendance et la composante saisonnière à partir de vos estimations. On note \\((R_j)_j\\) la série obtenue\n\n\nShow the code\nRj = Xt - P.est - S.est\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 4 : La série \\((R_j)_j\\) peut elle être modélisée par un bruit blanc ? Discuter le résultat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\non a l’impression d’avoir un bruit u peu centré \\((E(R_j)=0)\\) mais pas dingue avec une variance pas super constante. Mais, avec l’acf, ont voit tout de meme une corrélation faible qui pourrait faire pensé à un BB.\nCONCLUSION : ??\nNormalité (Test shapiro)\n\n\nShow the code\nshapiro.test(Rj)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Rj\nW = 0.98218, p-value = 0.05807\n\n\nVérification par le test de Shapiro-Wilk (normalité : p-value &gt; 0,05) On garde l’hypothèse de la normalité\nCorrélation\n\n\nShow the code\ncor.test(1:length(Rj), Rj)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  1:length(Rj) and Rj\nt = 2.1745, df = 142, p-value = 0.03133\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.01641902 0.33330168\nsample estimates:\n      cor \n0.1795128 \n\n\nLes deux valeurs importantes de ce résultat sont la corrélation (50%) et sa p-value (1*10^-8%). Le test nous dit en outre qu’il y a 95% de chances que la “véritable corrélation” entre temps et valeur soit comprise dans “l’intervalle de confiance” entre 36% et 61%. Notez que 0% est un nombre non inclus dans cette fourchette: en d’autres mots, il se pourrait très bien que valeur et temps soient du tout corrélés. En bref, l’algorithme détecte une corrélation.\nLe test de la statisticienne finnoise Greta Ljung (et de son directeur de thèse George Box) examine l’autocorrélation d’une série temporelle. Son “hypothèse nulle” est d’avoir affaire à du “bruit blanc”.\n\n\nShow the code\nBox.test(Rj, type=\"Ljung-Box\") \n\n\n\n    Box-Ljung test\n\ndata:  Rj\nX-squared = 26.159, df = 1, p-value = 3.145e-07\n\n\nLa p-value est faible (8e-09). Le test nous permet donc de rejeter l’hypothèse d’avoir affaire à du bruit blanc\n\n\nQUESTION 5 : Éliminer la tendance et la composante saisonnière en appliquant un ou plusieurs filtres linéaires de la forme \\((I − L^s)^d\\). On note \\((R'_j)_j\\) la série obtenue.\nEtant donné que l’on a détecter une tendance et une saisonnalité de période 12, on peut tenter une différenciation de la série avec \\(s=12\\) et \\(d=1\\).\n\n\nShow the code\nR1 = diff(Xt, lag=12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npour enlever le pb en 12, on se propose d’appliquer à nouveau la différence\n\n\nShow the code\nR2 = diff(R1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 6 : La série \\((R'_j)_j\\) peut elle être modélisée par un bruit blanc ? Discuter le résultat\n\n\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-28\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n latex2exp * 0.9.6   2022-11-28 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Fiche_04.html",
    "href": "posts/Fiche_04.html",
    "title": "Fiche 04",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\nEXERCICE 3 : \n\n\nQUESTION 2 : Tracer les valeurs des l’ACF \\(\\rho(h)\\) pour \\(h = 1, 2\\) . Vérifier vos résultats en utilisant la fonction ARMAacf.\n\n\n\nShow the code\n# On pose nos paramètres\na1 = c(1.6, -0.4, -1.2)\na2 = c(0.64, -0.45, 0.85)\n\n\n\n\nShow the code\nARMAacf(ar = c(-a1[1], -a2[1]), ma = 0, lag.max = 2, pacf = FALSE) \n\n\n         0          1          2 \n 1.0000000 -0.9756098  0.9209756 \n\n\nShow the code\nARMAacf(ar = c(-a1[2], -a2[2]), ma = 0, lag.max = 2, pacf = FALSE)\n\n\n        0         1         2 \n1.0000000 0.7272727 0.7409091 \n\n\nShow the code\nARMAacf(ar = c(-a1[3], -a2[3]), ma = 0, lag.max = 2, pacf = FALSE)\n\n\n          0           1           2 \n 1.00000000  0.64864865 -0.07162162 \n\n\nShow the code\n# lag.max = n fait calculer et afficher les n premières valeurs en partant de 0\n\n\nPour \\(h = 1, 2\\) on retrouve bien les valeurs calculées à la question 1.\n\n\nBONUS :\n\nOn se propose de Généraliser en comparant les fonctions calculés en question 1 avec ARMAacf et la fonction acf de r, \\(\\forall h\\).\n\n\nShow the code\n# On code notre fonction AR(2)\nn = 100\n\n\nAR2 = function(n, a, b) {\n  eps = rnorm(n + 100)  \n  x = rnorm(n + 100) #c'est pour donner la taille mais après on remplacera toute les valeurs\n  # on suppose que X_0 est une rnorm\n  for (i in (3:(n + 100))) {\n    x[i] = eps[i] - a * x[i - 1] - b * x[i - 2]\n  }\n  X_final = x[101:(n + 100)]\n  return(X_final)  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# On code des fonction pour définir \\rho à partir des calculs de la question 1\n\nrho1 = function(h){\n  r = (-5/4)^(-h) * (1 + h * (9/41))\n  return(r)\n}\n\nrho2 = function(h){\n  r = (135/154) * (9/10)^h + (19/154) * (-1)^h * (1/2)^h\n  return(r)\n}\n\nrho3 = function(h){\n  mod_z1 = sqrt(340/289)\n  arg_z1 = atan(7/6)\n  A = ( ( (24/37) * sqrt(30/17) - cos(atan(7/6)) / sin(atan(7/6))) )^2\n  c1 = -sqrt(1+A)\n  c2 = acos(1/c1)\n  r = c1 * mod_z1^(-h) * cos(h * arg_z1 + c2)\n  return(r)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCICE 5 : \n\n\nQUESTION 3 : Pour les modèles AR(2) définis à l’exercice 3, Simuler une trajectoire (avec \\(n=100\\)) pour les 3 modèles, puis calculer et représenter le prédicteur pour \\(h=1,...,10\\) (avec un intervalle de confiance).\n\nOn se base sur les 3 processus AR(2) définit en questions 3\n\n\nShow the code\n# fonction pour simuler et prévoir\nsimu_and_pred = function(coef1, coef2){\n  # simulations\n  X_t = AR2(n, coef1, coef2) # On utilise notre fonction AR2\n  \n  # prévisions\n  Prev = vector(\"numeric\", length = 12)\n  Prev[1] = X_t[n-1] # horizon h = -1\n  Prev[2] = X_t[n] # horizon h = 0\n  \n  # horizon h = 1, ..., 10\n  for(k in 3:12){\n    Prev[k] = -coef1 * Prev[k-1]  -coef2 * Prev[k-2]\n  }\n  \n  # erreurs\n  \n  # ARMAtoMA donne (psi_1, ..., psi_lag.max) :\n  # il faut donc ajouter psi_0 = 1\n    erreurs = cumsum(c(1, ARMAtoMA(ar = c(-coef1,- coef2), lag.max = 9)^2))\n  \n  # Intervalles de prévision :\n  \n  # IC à 95% \n  alpha_95 = 0.05\n  q_alpha_95 = qnorm(1 - alpha_95/2)\n  # bornes inf des intervalles de prévison\n  l_95 = Prev[3:12] - q_alpha_95 * sqrt(erreurs) \n  # bornes sup des intervalles de prévison\n  u_95 = Prev[3:12] + q_alpha_95 * sqrt(erreurs) \n  \n  # IC à 80%\n  alpha_80 = 0.2\n  q_alpha_80 = qnorm(1 - alpha_80/2) \n  l_80 = Prev[3:12] - q_alpha_80 * sqrt(erreurs) \n  u_80 = Prev[3:12] + q_alpha_80 * sqrt(erreurs) \n  \n  return(list(X_t = X_t, \n              Prev = Prev[3:12], \n              lower = list(l_80 = l_80, l_95 = l_95),\n              upper = list(u_80 = u_80, u_95= u_95)))\n}"
  },
  {
    "objectID": "posts/Fiche_02.html",
    "href": "posts/Fiche_02.html",
    "title": "Fiche 02",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\nlibrary(latex2exp)\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n latex2exp * 0.9.6   2022-11-28 [1] CRAN (R 4.2.3)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\nEXERCICE 1 : \n\nNous étudions la différence entre une marche aléatoire et un signal linéaire bruité. \n\nQUESTION 1 : Simuler dix marches aléatoires \\((x_t)_t = \\delta + x_{t-1} + w_t\\) avec dérive de longueur \\(n = 100\\), de paramètre \\(\\delta = .01\\) et de variance \\(\\sigma^2_W = 1\\) pour le bruit.\n\nSi on pose que \\(x_0 = w_0\\), on peut écrire notre marche aléatoire comme \\(x_t = \\delta t + \\sum_{i=0}^{t}w_i\\)\n\n\nShow the code\n# On pose nos paramètres\nn = 100         \ndelta = 0.01    \n\n# On définis notre fonction de marche aléatoire\nrandom_walk = function(n, delta) {\n  w = rnorm(n)  \n  drift = delta * seq(1, n)  \n  \n  x = drift + cumsum(w)\n  return(x)  \n}\n\n# Générer dix marches aléatoires\nnb = 10\nsim = matrix(0, ncol = n, nrow = nb)\nfor (i in 1:nb) {\n  sim[i, ] = random_walk(n, delta)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 2 : Estimer le modèle de régression linéaire \\(x_t = \\beta_t + w_t\\)\n\n\n\nShow the code\nlist=1:n\nl = c()\n\nfor (i in 1:nb){\n  mod = lm(sim[i,] ~ list + 0) \n  # +0 pour ne pas faire de modèle avec constante\n  l[i] = mod$coefficients\n}\n\n\n\n\n\nQUESTION 3 : Représenter sur un même graphique les dix droites estimées et la tendance moyenne théorique \\(\\delta_t = .01t\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 4 : Simuler dix séries \\((x_t)_t\\) de la forme \\(x_t = \\delta_t + w_t\\) (tendance+bruit blanc) de longueur \\(n = 100\\), de paramètre \\(\\delta = .01\\) et de variance \\(\\sigma^2_W = 1\\)\n\n\n\nShow the code\n# On définis notre signal linéaire bruité \nnoisy_serie = function(n, delta) {\n  w = rnorm(n, sd = 1)  \n  drift = delta * seq(1, n) \n  \n  x = drift + w\n  return(x)  \n}\n\n# Générer dix séries tendance + bruit\nsim2 = matrix(0, ncol = n, nrow = nb)\n\nfor (i in 1:nb) {\n  sim2[i, ] = noisy_serie(n, delta)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 5 : Estimer le modèle de régression linéaire \\(x_t = \\beta_t + w_t\\)\n\n\n\nShow the code\nl2 = c()\nfor (i in 1:nb){\n  mod2 = lm(sim2[i,] ~ list + 0)\n  l2[i] = mod2$coefficients\n}\n\n\n\n\n\nQUESTION 6 : Représenter sur un même graphique les dix droites estimées et la tendance théorique \\(\\delta_t = .01t\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 7 : Commenter les résultats\n\nLa tendance théorique (le drift) est mieux estimée par régression dans le cas d’un signal bruité que celui de la marche aléatoire.\nCela peut s’explique par le fait que, dans le cas de la marche aléatoire, la variance de \\(x_t\\) croît linéairement avec le temps. En effet, \\(Var(x_t) = Var( \\delta t + \\sum_{i=0}^{t}w_i) = Var( \\sum_{i=0}^{t}w_i)=\\sum_{i=0}^{t}Var(w_i) = \\sum_{i=0}^{t}\\sigma^2_w=t\\sigma^2_w\\)\nCela fait donc défaut à l’hypothèse d’homoscédacité cruciale pour la régression linéaire\nPar contre, du côté du signal bruite on conserve l’homoscédacité avec le cas très idéal du bruit iid et gaussien. \n\n\n\nEXERCICE 2 : \n\n\nQUESTION 1 : Écrire une fonction qui retourne une série simulée de la forme \\(X_j = a cos(ω_j) + bj + \\varepsilon_j\\) où \\((\\varepsilon_n)\\) un bruit blanc gaussien centré et de variance 1.\n\nLes paramètres d’entrée de la fonction sont \\(n\\), \\(a\\), \\(b\\), \\(w\\) et la sortie est une série temporelle. Pour cela, on utilise la fonction ts() qui transforme nos points générés en une série temporelle.\n\n\nShow the code\nX_j = function(n, a, b, w) {\n  eps = rnorm(n)  \n  \n  x = a*cos(w*seq(1, n) ) + b* seq(1, n) +eps\n  return(ts(x))   \n}\n\n\nMaintenant, on fixe \\(n = 100\\) puis \\(n = 500\\)\n\n\nShow the code\nn = c(100, 500)\n\n\nPuis on pose les paramètres qui nous serons utiles par la suite\n\n\nShow the code\na = c(0, 2)\nb = c(0.01, 0)\nw = c(2*pi, pi/6) \n#en 2*pi, w n'aura pas d'influence si on voulait enlever la condition a = 0\n\n\n\n\n\nQUESTION 2 : Pour \\(a = 0\\) et \\(b = .01\\), simuler une trajectoire, puis représenter\n\n\n\nShow the code\n# Pour n = 100\nsim1_X_j_100 = X_j(n[1], a[1], b[1], w[1])\n\n# Pour n = 500\nsim1_X_j_500 = X_j(n[2], a[1], b[1], w[1])\n\n\n\n\n\n2-1 : la série et sa suite d’auto-corrélations empiriques\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2-2 : la série \\(X_n − X_{n−1}\\) et sa suite des auto-corrélations empiriques\n\n\n\nShow the code\nsim1_X_j_100_diff = diff(sim1_X_j_100, lag = 1)\n\nsim1_X_j_500_diff = diff(sim1_X_j_500, lag = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 3 : Pour \\(b = 0\\), \\(a = 2\\) et \\(w = \\frac{\\pi}{6}\\), simuler une trajectoire, puis représenter\n\n\n\nShow the code\n# Pour n = 100\nsim2_X_j_100 = X_j(n[1], a[2], b[2], w[2])\n\n# Pour n = 500\nsim2_X_j_500 = X_j(n[2], a[2], b[2], w[2])\n\n\n\n\n\n3-1 : la série et sa suite des auto-corrélations empiriques\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3-2 : la série \\(X_n − X_{n−12}\\) et sa suite des auto-corrélations empiriques\n\n\n\nShow the code\nsim2_X_j_100_diff = diff(sim2_X_j_100, lag = 12)\n\nsim2_X_j_500_diff = diff(sim2_X_j_500, lag = 12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour conclure, on peut constater qu’en grandissant l’échantillon, la tendance et la saisonnalité ressortent d’avantage et influencent nos autocorrélation. On ne pourra donc pas considérer les séries comme stationnaires. Mais, l’opération de différenciation peut permettre de résoudre ce problème d’autocorrélation quand celle ci est adaptée à la “perturbation”(tendance ou saison) de notre série.\nPour visualiser cela, on peut effecuer les calculs. Dans le cas de la série \\(X_j = 0.01j +\\varepsilon_j\\) Où il reste l’effet de tendance, \\[\\begin{align*}\nX_j - X_{j-1} &= 0.01j +\\varepsilon_j - 0.01(j-1) -\\varepsilon_{j-1}\\\\\n&= 0.01j - 0.01j - 0.01  + \\varepsilon_j  - \\varepsilon_{j-1}\\\\\n&= -0.01  +\\varepsilon_j  - \\varepsilon_{j-1}\\\\\n\\end{align*}\\] On a bien une disparition de la tendance.\nDans le cas de la série \\(X_j = 2cos(\\frac{\\pi}{6}j) + \\varepsilon_j\\) Où il reste l’effet de saison, \\[\\begin{align*}\nX_j - X_{j-12} &= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}(j-12)) -\\varepsilon_{j-12}\\\\\n&= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}j-\\frac{\\pi}{6}12) -\\varepsilon_{j-12}\\\\\n&= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2cos(\\frac{\\pi}{6}j-2\\pi) -\\varepsilon_{j-12}\\\\\n&= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2(cos(\\frac{\\pi}{6}j)cos(2\\pi) - sin(\\frac{\\pi}{6}j)sin(2\\pi)) -\\varepsilon_{j-12}\\\\\n&= 2cos(\\frac{\\pi}{6}j) +\\varepsilon_j - 2(cos(\\frac{\\pi}{6}j) - 0) -\\varepsilon_{j-12}\\\\\n&= \\varepsilon_j -\\varepsilon_{j-12}\\\\\n\\end{align*}\\] On a bien une disparition de la saison.\n\nOn a donc qu’une différentiation d’ordre 1 permettra d’enlever la tendance et une différentiation d’ordre \\(s\\) permettra d’enlever une saison de période \\(s\\).\n\n\n\n\nEXERCICE 3 : \n\n\nQUESTION 1 : Ecrire une fonction pour simuler des trajectoires de processus défini par l’équation de récurrence \\(X_m + cX_{m−1} = \\varepsilon_m\\) où \\((\\varepsilon_m)\\) est une suite de variables aléatoires centrées iid.\n\nIndication : Pour obtenir une série de longueur \\(m\\), simuler \\(m+ 100\\) valeurs et supprimer les 100 premières valeurs pour atténuer l’effet de l’initialisation. Vous pouvez utiliser la fonction filter.\n\n\nShow the code\nxt = function(m, c) {\n  eps = rnorm(m + 100)  \n  x = rep(NA, m + 100) \n  # On suppose pour notre condition initial\n  x[1] = eps[1] \n  \n  for (i in (2:(m + 100))) {\n    x[i] = eps[i] - c * x[i - 1]\n  }\n  \n  x_final = x[101:(m + 100)]\n  return(ts(x_final))  \n}\n\n\n\n\n\nQUESTION 2 : Pour \\(|c| = 0, .5, .9\\), tracer une trajectoire simulée et sa suite des auto-corrélations empiriques\n\n\n\nShow the code\n# On pose nos paramètres\nm = 500\nc = c(-0.9, -0.5, 0, 0.5, 0.9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 3 : Commenter les résultats\n On remarque qu’au moment où nos paramètres sont proche de \\(1\\) ou \\(-1\\), nos autocorrélations sont forte et notre série perd en stationnarité. En effet, on remarque que le processus est un AR(1) avec son acf qui décroit exponentiellement et la stationnarité se perd quand \\(|c| \\longrightarrow 1\\).\nOn remarque également que, qand \\(c=0\\), on a un bruit blanc.\n\n\n\nBONUS\n\nEn Complément de ces informations, on peut aussi s’interesser au PACF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn reconnait alors les caractéristiques d’un AR(1) au vu des ACF et PACF. Et le cas de \\(c=0\\) apparait plus clairement comme celui d’un bruit blanc. \n\n\n\nEXERCICE 4 : \n\nLe fichier champ.asc est disponible sur le web à l’adresse suivante http://www.math.sciences.univ-nantes.fr/~philippe/lecture/champ.asc\nOn note \\((C_t)\\) la série.\n\n\nShow the code\nurl = \"http://www.math.sciences.univ-nantes.fr/~philippe/lecture/champ.asc\"\ndata = read.csv(url)\n\nCt = ts(data)\n\n\n\n\nQUESTION 1 : Tracer la série \\((C_t)\\) et sa suite des auto-corrélations empiriques\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 2 : En utilisant les résultats de l’exercice précédent, peut-on détecter la présence d’une fonction périodique ou d’une tendance dans cette série.\n D’après les graphiques obtenus ainsi que l’analyse faite à l’exercice précédent, on constate clairement une série de type multplicatif (variance qui explose avec le temps) présentant une tendance linéaire et une saisonnalité (de période environ 12 peut-être). \n\n\nQUESTION 3 : Tracer la série \\((log(Ct))\\) et sa suite des auto-corrélations empiriques\n\n\n\nShow the code\nlCt = log(Ct)\n\n\n\n\n\n\n\n\n\n\n\nOn peut observé que le passage au log à permis de contrer la croissance en \\(t\\) de la variance.  En effet, si on pose \\(Y_t = t\\varepsilon_t\\), alors \\(Var(Y_t) = t^2Var(\\varepsilon_t)\\).  Or, avec le passage au log, on aura que \\(Var(log(Y_t)) = Var(log(t\\varepsilon_t)) = Var(log(t)+log(\\varepsilon_t)) = Var(log(\\varepsilon_t))\\)\n\n\n\nQUESTION 4 : Pour différentes valeurs des paramètres \\((\\alpha, \\beta, \\gamma)\\), simuler les séries suivantes de longueur 100 où \\((\\varepsilon_t)\\) est une suite de variables aléatoires i.i.d. \\(\\mathcal{N}_{(0, 1)}\\)\n On va donc simuler des séries de la forme suivante \\[\\begin{align}\n\\alpha t + \\beta cos(\\frac{2πt}{12}) + \\gamma cos(\\frac{2πt}{6}) + \\beta' cos(\\frac{2πt}{12}) + \\gamma' cos(\\frac{2πt}{6}) + \\varepsilon_t\n\\end{align}\\]\n\n\nShow the code\nut = function(n, a, b, c, d, e, a0 = 0){\n  t = 1:n\n  eps_t = rnorm(n)\n  \n  u = a0 + a*t + b*cos(pi/6*t)  + c*cos(pi/3*t) + d*sin(pi/6*t)  + e*sin(pi/3*t) + eps_t \n  return(u)\n}\n\n# Simulations pour différentes valeurs de coefficients\nn = rep(100,8)\nalpha = rep(c(0.01, 0.05),4)\nbeta = rep(c(-1,1,0.1,2),2)\ngamma = rep(c(-0.1,1,2,-0.5),2)\nd = gamma\ne = beta\na0 = rep(c(0, 7.5, 8, 8.5),2)\n\n\n# On stocke les simulations \nut.vect = Vectorize(ut)\nsimu.res = ut.vect(n,alpha,beta,gamma,d,e,a0)\n\n\nOn peut déjà constater que cette série à été construite dans l’optique de prendre les périodes que l’on peut détecter avec l’ACF autour du Lag 6 et du Lag 12 et qui sont adéquate à l’aspect de “double pics” présent dans notre série. Le terme \\(\\alpha\\) est de son côté, présent pour prendre en compte la présence de la tendance linéaire.\n\n\n\nQUESTION 5 : Comparer l’allure des séries simulées avec la série des ventes de champagne et la série \\((log(Ct))\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 6 : Pour laquelle des deux séries \\(((Ct))\\) \\((log(Ct))\\), le modèle défini en question 4 vous semble le plus pertinent.\n\nLa série \\(log(C_t)\\) est la plus adaptée à l’halure des séries simulées car, pour les series simulées, on a pas la variance qui augmente comme pour\\(C_t\\). \n\n\nQUESTION 7 : Sur cette série, calculer les estimateurs de \\((\\alpha, \\beta, \\gamma)\\) par la méthode des moindres carrés. Que peut-on dire de la qualité du modèle. Peut on modéliser la série des résidus par un bruit blanc?\n\n\n\nShow the code\n#construisons un modèle de regression\nt = seq(1,length(Ct))\n\nmodel = lm(log(Ct) ~ t + cos((pi/6)*t)  + cos((pi/3)*t) + sin((pi/6)*t)  + sin((pi/3)*t))\nsummary(model)\n\n\n\nCall:\nlm(formula = log(Ct) ~ t + cos((pi/6) * t) + cos((pi/3) * t) + \n    sin((pi/6) * t) + sin((pi/3) * t))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.76107 -0.13260  0.00618  0.18329  0.48198 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      8.1452854  0.0512649 158.886  &lt; 2e-16 ***\nt                0.0043032  0.0008477   5.077 1.83e-06 ***\ncos((pi/6) * t)  0.2760137  0.0360024   7.667 1.30e-11 ***\ncos((pi/3) * t) -0.1698754  0.0361401  -4.700 8.49e-06 ***\nsin((pi/6) * t) -0.2165525  0.0360024  -6.015 3.11e-08 ***\nsin((pi/3) * t) -0.3990958  0.0357974 -11.149  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2592 on 98 degrees of freedom\nMultiple R-squared:  0.7243,    Adjusted R-squared:  0.7103 \nF-statistic:  51.5 on 5 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nOn constate une forte significativité de tout nos termes."
  },
  {
    "objectID": "posts/Fiche_06Bootstrap.html",
    "href": "posts/Fiche_06Bootstrap.html",
    "title": "Fiche 06 : Bootstrap",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nShow the code\n# Données\nlibrary(dplyr)        # manipulation des données\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nFonction 1Fonction 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDonnées\n\n\nAnalyse\n\n\n\n\n\n\nNote\n\n\n\nMETTRE LES REMARQUES\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMETTRE LES POINTS D’ATTENTION\n\n\n\n\n\n\nRésultats\n\n\nMETTRE LES CONCLUSIONS\n\n\n\nConclusion\n\n\nSession info\n\n\nShow the code\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-27\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\nOn considère le modèle \\[\\begin{align}\nx_t = \\mu + \\phi(x_{t-1} - \\mu) + w_t\n\\end{align}\\]\n\\(\\longrightarrow\\) Les valeurs des paramètres sont \\(\\mu = 50\\) et \\(\\phi = .95\\)\n\\(\\longrightarrow\\) On dispose d’un échantillon de longueur \\(n = 100\\).\n\\(\\longrightarrow\\) Hypothèse sur le bruit : la suite \\((w_t)_t\\) est une suite de va iid suivant la loi double exponentielle, c’est à dire la loi de densité \\[\\begin{align}\nf(x) = \\frac{1}{4}e^{-\\frac{|x|}{2}}\n\\end{align}\\]\nOn estime le paramètre \\(\\phi\\) par l’estimateur de Yule Walker \\(\\hat{\\phi}_n\\). On peut calculer cet estimateur à l’aide de la fonction ar.yw.\n\nQUESTION 1 : Mettre en oeuvre un générateur de nombres aléatoires suivant la loi du bruit.\nIndication : Montrer que \\(w_1 = XZ −X(1−Z)\\) où la loi de \\(X\\) est la loi exponentielle de paramètre \\(\\frac{1}{2}\\) et la loi de \\(Z\\) est la loi de Bernoulli de paramètre \\(\\frac{1}{2}\\). \\(X\\) et \\(Z\\) sont indépendantes.\nNous commençons par mettre en oeuvre un générateur de nombres aléatoires suivant la loi du bruit.\n\n\nShow the code\nW=function(n) {\n  x=rexp(n, 1/2)\n  z=rbinom(n, 1, 1/2)\n  return(x * z - x * (1 - z))\n}\n\n\n\n\nQUESTION 2 : Construire une fonction pour simuler des processus AR(1) suivant le modèle considéré.\n(vous pouvez utiliser la fonction arima.sim, prendre n.start = 50 pour supprimer les 50 premières observations )\nNous voulons maintenant pouvoir simuler un processus AR(1) suivant notre modèle.\n\n\nShow the code\nsimu_ar=function(mu, phi, n) {\n  X=c()\n  X[1] = W(1)\n  for (i in 2:(n + 50)) {\n    X[i] = mu + phi * (X[i - 1] - mu) + W(1)\n  }\n  return(X[51:(n + 50)])\n}\n\n\n\n\nQUESTION 3 : En utilisant une méthode de Monte Carlo construire un échantillon suivant la loi de \\(\\hat{\\phi}_n - \\phi\\) et représenter graphiquement la densité de la loi de \\(\\hat{\\phi}_n - \\phi\\) approchée à partir de cet échantillon.\nNous souhaitons utiliser une méthode de Monté-Carlo pour construire un échantillon qui suit la loi de \\(\\hat{\\phi}_n - \\phi\\).\n\n\nShow the code\n# nombre d'itérations pour notre méthode MC\nnb_iterations = 1000\n\n# Echantillon pour stoccker les \\hat{\\phi}_n - \\phi\nechantillon_phi=rep(0, nb_iterations)\n\n# Matrice pour stocker nos simulations d'où l'on prélèvera le \\phi estimé\nx=matrix(NA,nrow = nb_iterations,ncol=nb_iterations)\n\nfor (i in 1:nb_iterations) {\n  x[i,] = simu_ar(mu, phi, n)\n  \n  # Estimer phi avec Yule-Walker\n  phi_n = ar.yw(x[i,], order = 1)$ar\n  \n  \n  echantillon_phi[i] = phi_n - phi\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 4 : Simuler votre échantillon d’observations\nOn simule un échantillon d’observations.\n\n\nShow the code\nechant = simu_ar(mu,phi,n)\n\n\n\n\nQUESTION 5 : Estimer le paramètre \\(\\phi\\). Représenter graphiquement la densité de l’approximation asymptotique gaussienne de la loi de \\(\\hat{\\phi}_n - \\phi\\)\nPour cette estimation, on utilise le fait que \\[\\begin{align}\n\\hat{\\phi}_n - \\phi \\underset{n\\to +\\infty}{\\longrightarrow} \\mathcal{N}_{(0, \\sigma^2)}\n\\end{align}\\]\nPuis, grâce au théorème de Slutsky, on a \\[\\begin{align}\n\\frac{\\hat{\\phi}_n - \\phi}{\\hat{\\sigma}} \\underset{n\\to +\\infty}{\\longrightarrow} \\mathcal{N}_{(0, 1)}\\\\\n\\end{align}\\]\nOn peut donc estimer \\(\\sigma^2\\) puis utiliser cette estimation pour proposer une représentation de l’approximation asymptotique gaussienne de la loi de \\(\\hat{\\phi}_n - \\phi\\)\nNous voulons faire une approximation gaussienne de la densité de \\(\\hat{\\phi}_n - \\phi\\). Pour cela, nous estimons \\(\\phi\\) grâce à l’estimateur de Yule Walker. Grâce à cet estimateur, nous avons également une estimation de la variance. Avec cette dernière, nous pouvons alors simuler une loi gaussienne \\(\\mathcal{N}(0,\\hat\\sigma^2)\\) où \\(\\hat\\sigma^2\\) est l’estimateur de la variance.\n\n\nShow the code\nesti = ar.yw(echant, order = 1)\n\nphi_hat = esti$ar # Estimateur de phi\nsigma_hat = sqrt(esti$asy.var.coef) # Estimateur de l'écart type \n\n\nPour vérifier que l’on a une bonne estimation, nous pouvons comparer à la densité obtenue par Monté-Carlo.\n\n\nShow the code\nhist(echantillon_phi, \n  ylab = TeX(\"$phi^n - phi$\"),\n  xlab=\"\",\n  probability = TRUE, \n  col='cyan',\n  main=\"Comparaison des méthodes\")\ncurve(dnorm(x, 0, sigma_hat), add = TRUE, lty = 2, col = \"red\")\n#lines(seq(-.05, .03, length.out=1000),normal, col=\"red\", lty=1)\nlegend(\"topleft\", legend=c(\"Méthode MC\",\"Approximation gaussienne\"),\n       fill=c(\"cyan\",NA),lty=c(0,2),col=c(NA,\"red\"), border=c(\"black\",NA), \n       pch=c(22,NA))\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 6 : Mettre en oeuvre le bootstrap non paramétrique sur les résidus. A partir de 500 échantillons bootstrappés construire une approximation de la loi de \\(\\hat{\\phi}_n - \\phi\\). Représenter graphiquement l’estimation de la densité de la loi de \\(\\hat{\\phi}_n - \\phi\\)\nLa méthode utilisée est détaillée dans le CM\nNous souhaitons mettre en oeuvre un bootstrap non paramétrique sur nos résidus.\n\n\nShow the code\n# Phi estimé\nphi_est = ar.yw(echant, order=1)$ar\n\n# On récupère les résidus pour faire le bootstrap\nres = echant\n\n# Il faut récupérer les résidus \nfor (i in 2:n){\n  res[i] = echant[i]-phi_est*echant[i-1]\n}\n\nn_bootstrap = 500\nphi_hat = numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap){\n  w_etoile = sample(res, replace=T) # Loi uniforme (def bootstrap)\n  \n  # On simule de nouveau un AR(1) de notre modèle mais avec nos résidus estimés\n  test = arima.sim(list(ar=phi),n=n_bootstrap, innov=w_etoile,n.start=50) + mu \n  \n  # On estime pour faire la représentation graphique \n  phi_hat[i] = ar.yw(test, order=1)$ar - phi \n  \n}\n\n\nSinon, use le syst dans CM\n\\[\\begin{align}\nX_t^* = X_t\nX_{t+1}^* = mu + \\hat{\\phi_n}() + w_t^*\n\n\\end{align}\\]\npour ne pas avoir a faire la boucle de création de résidus et avoir seulement une boucle.\n\n\nQUESTION 7 : Comparer les deux approximations (gaussienne et bootstrap) à la loi calculée par la méthode de Monte Carlo (que l’on peut comme la loi exacte aux approximations numériques près)\nNous souhaitons comparer nos différentes méthodes:\n\n\nShow the code\nhist(echantillon_phi, \n  ylab = TeX(\"$phi^n - phi$\"),\n  xlab=\"\",\n  probability = TRUE, \n  col='cyan',\n  main=\"Comparaison des méthodes\")\ncurve(dnorm(x, 0, sigma_hat), add = TRUE, lty = 2, col = \"red\")\nlines(density(phi_hat), col='lightgreen', lty=1, lwd = 2 )\nlegend(\"topleft\", legend=c(\"Méthode MC\",\"Approximation gaussienne\", \"Boostrap\"),\n       fill=c(\"cyan\",NA,NA),lty=c(0,1,1),col=c(NA,\"red\",\"lightgreen\"), pch=c(22,NA,NA), border=c(\"black\",NA,NA), lwd = c(0,1,2))\n\n\n\n\n\n\n\n\n\n\n\nQUESTION 8 : En utilisant vos échantillons bootstrappés donner une approximation de l’intervalle de prévision à l’horizon 1 de niveau 95% .\nNous voulons, à partir de nos échantillons bootstrappés, donner une approximation de l’intervalle de prévision à l’horizon 1 de niveau 95%.\nOn reprend notre méthode de bootstrap en ajoutant une étape de prévision\n\n\nShow the code\n# Phi estimé\nphi_est = ar.yw(echant, order=1)$ar\n\n# On récupère les résidus pour faire le bootstrap\nres = echant\n\n# Il faut récupérer les résidus \nfor (i in 2:n){\n  res[i] = echant[i]-phi_est*echant[i-1]\n}\n\nn_bootstrap = 500\nphi_hat = numeric(n_bootstrap)\n\n\nprev = rep(NA, n_bootstrap)\nphi_etoile = rep(NA, n_bootstrap)\n\nfor (i in 1:n_bootstrap){\n  w_etoile = sample(res, replace=T) # Loi uniforme (def bootstrap)\n  \n  # On simule de nouveau un AR(1) de notre modèle mais avec nos résidus estimés\n  test = arima.sim(list(ar=phi), n=n, innov=w_etoile,n.start=50) + mu \n  \n  phi_etoile[i] = ar.yw(test, order=1)$ar\n  \n  # Prévision\n  prev[i] =  phi_etoile[i] *test[n]\n}\n\n\n\n\nShow the code\n# ecart-type de l'erreur de prévision approximé par bootstrap\nsigma.prev.boot = sd(prev)\n\nprint(paste(\"ecart-type de prévision à h=1 : \", round(sigma.prev.boot, 2))\n)\n\n\n[1] \"ecart-type de prévision à h=1 :  9.84\"\n\n\nShow the code\n# l'intervalle de prévision approximé par bootstrap\nprev.yw = predict(esti, h=1)\nbornes = as.numeric(prev.yw$pred) - quantile(prev - as.numeric(prev.yw$pred),\n                                             c(0.975, 0.025))\n\nq_inf = bornes[1]\nq_sup = bornes[2]\nprint(paste(\"Intervalle bootstrap de prévision à h=1 : [\", round(q_inf,2), \",\", round(q_sup,2), \"]\"))\n\n\n[1] \"Intervalle bootstrap de prévision à h=1 : [ -81.52 , -44.69 ]\"\n\n\n\n\nQUESTION 9 : Mettre en oeuvre le bootstrap stationnaire et donner une approximation de l’intervalle de prévision à l’horizon 1.\n\n\nQUESTION 10 : Comparer les résultats obtenus par bootstrap avec l’intervalle théorique de prévision."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Séries temporelles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExercice 2.01\n\n\n5 min\n\n\n\nFiche 2\n\n\nMarche aléatoire\n\n\nSignal linéraire bruité\n\n\n\nL’objectif de ce document est détudier la différence entre une marche aléatoire et un signal linéraire bruité\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 12:24:08 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercie 3.1\n\n\n10 min\n\n\n\nFiche 3\n\n\nTendance\n\n\nSaisonnalité\n\n\n\nNous allons illustrer l’étude de la tendance et de la saisonnalité à partir d’une base de données sur la vente de voitures\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 1:47:36 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiche 02\n\n\n14 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 9:57:15 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiche 04\n\n\n11 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 9:57:15 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiche 05\n\n\n19 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 9:57:15 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiche 06\n\n\n8 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 9:57:15 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiche 06 : Bootstrap\n\n\n7 min\n\n\n\ncategorie 1\n\n\ncotegorie 2\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/28/25, 9:57:15 AM\n\n\n\n\n\n\n\nNo matching items"
  }
]